{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"Import all necessary libraries and install everything you need for training:","metadata":{}},{"cell_type":"code","source":"# install the libraries necessary for data wrangling, prediction and result analysis\nimport json\nimport numpy as np\nimport pandas as pd\nimport logging\nimport matplotlib.pyplot as plt\nfrom sklearn import metrics\nfrom sklearn.metrics import classification_report, confusion_matrix, f1_score,precision_score, recall_score\nimport torch\nfrom numba import cuda\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.dummy import DummyClassifier","metadata":{"execution":{"iopub.status.busy":"2022-08-23T06:49:11.306516Z","iopub.execute_input":"2022-08-23T06:49:11.307361Z","iopub.status.idle":"2022-08-23T06:49:15.445827Z","shell.execute_reply.started":"2022-08-23T06:49:11.307245Z","shell.execute_reply":"2022-08-23T06:49:15.444356Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"# Install transformers\n# (this needs to be done on Kaggle each time you start the session)\n!pip install -q transformers\n\n# Install the simpletransformers\n!pip install -q simpletransformers\nfrom simpletransformers.classification import ClassificationModel","metadata":{"execution":{"iopub.status.busy":"2022-08-23T06:49:28.705862Z","iopub.execute_input":"2022-08-23T06:49:28.707150Z","iopub.status.idle":"2022-08-23T06:50:08.593158Z","shell.execute_reply.started":"2022-08-23T06:49:28.707099Z","shell.execute_reply":"2022-08-23T06:50:08.592037Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"# Install wandb\n!pip install -q wandb\n\nimport wandb\n\n# Login to wandb\nwandb.login()","metadata":{"execution":{"iopub.status.busy":"2022-08-23T06:50:08.595800Z","iopub.execute_input":"2022-08-23T06:50:08.596781Z","iopub.status.idle":"2022-08-23T06:50:29.716857Z","shell.execute_reply.started":"2022-08-23T06:50:08.596738Z","shell.execute_reply":"2022-08-23T06:50:29.715516Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"# Clean the GPU cache\n\ncuda.select_device(0)\ncuda.close()\ncuda.select_device(0)\ntorch.cuda.empty_cache()\n","metadata":{"execution":{"iopub.execute_input":"2022-07-29T06:58:57.624647Z","iopub.status.busy":"2022-07-29T06:58:57.624034Z","iopub.status.idle":"2022-07-29T06:58:57.929939Z","shell.execute_reply":"2022-07-29T06:58:57.928897Z","shell.execute_reply.started":"2022-07-29T06:58:57.624608Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Import the data","metadata":{}},{"cell_type":"code","source":"# FTD\ntrain_df = pd.read_csv(\"/kaggle/input/xgenre-datasets/FTD-X-GENRE-train.txt\", sep=\"\\t\", index_col=0)\ndev_df = pd.read_csv(\"/kaggle/input/xgenre-datasets/FTD-X-GENRE-dev.txt\", sep = \"\\t\", index_col = 0)\ntest_df = pd.read_csv(\"/kaggle/input/xgenre-datasets/FTD-X-GENRE-test.txt\", sep = \"\\t\", index_col = 0)\n\nprint(\"FTD train shape: {}, Dev shape: {}, Test shape: {}.\".format(train_df.shape, dev_df.shape, test_df.shape))","metadata":{"execution":{"iopub.status.busy":"2022-08-23T06:50:35.256090Z","iopub.execute_input":"2022-08-23T06:50:35.256549Z","iopub.status.idle":"2022-08-23T06:50:35.522198Z","shell.execute_reply.started":"2022-08-23T06:50:35.256507Z","shell.execute_reply":"2022-08-23T06:50:35.521102Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"train_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-08-23T06:50:38.328838Z","iopub.execute_input":"2022-08-23T06:50:38.329317Z","iopub.status.idle":"2022-08-23T06:50:38.352724Z","shell.execute_reply.started":"2022-08-23T06:50:38.329275Z","shell.execute_reply":"2022-08-23T06:50:38.351792Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"## Training and saving","metadata":{}},{"cell_type":"markdown","source":"We will use the multilingual XLM-RoBERTa model\nhttps://huggingface.co/xlm-roberta-base","metadata":{}},{"cell_type":"code","source":"import os\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"","metadata":{"execution":{"iopub.status.busy":"2022-08-23T06:50:50.537260Z","iopub.execute_input":"2022-08-23T06:50:50.538134Z","iopub.status.idle":"2022-08-23T06:50:50.543141Z","shell.execute_reply.started":"2022-08-23T06:50:50.538095Z","shell.execute_reply":"2022-08-23T06:50:50.541836Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"# Create a list of labels\nLABELS = train_df.labels.unique().tolist()\nLABELS","metadata":{"execution":{"iopub.status.busy":"2022-08-23T06:50:53.384709Z","iopub.execute_input":"2022-08-23T06:50:53.385733Z","iopub.status.idle":"2022-08-23T06:50:53.395849Z","shell.execute_reply.started":"2022-08-23T06:50:53.385694Z","shell.execute_reply":"2022-08-23T06:50:53.394664Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"# Initialize Wandb\nwandb.init(project=\"X-GENRE classifiers\", entity=\"tajak\", name=\"FTD-hyperparameter-search\")","metadata":{"execution":{"iopub.status.busy":"2022-08-23T06:51:46.307893Z","iopub.execute_input":"2022-08-23T06:51:46.308594Z","iopub.status.idle":"2022-08-23T06:51:49.218144Z","shell.execute_reply.started":"2022-08-23T06:51:46.308547Z","shell.execute_reply":"2022-08-23T06:51:49.217183Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"# Calculate how many steps will each epoch have\n# Num steps in epoch = training samples / batch size\nsteps_per_epoch = int(630/8)\nsteps_per_epoch","metadata":{"execution":{"iopub.status.busy":"2022-08-23T06:52:06.594645Z","iopub.execute_input":"2022-08-23T06:52:06.595840Z","iopub.status.idle":"2022-08-23T06:52:06.605467Z","shell.execute_reply.started":"2022-08-23T06:52:06.595787Z","shell.execute_reply":"2022-08-23T06:52:06.604437Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"I evaluated per every 10th epoch - per 780 steps. I first trained the model while evaluating it to find the optimal number of epochs and then trained it again and saved it.","metadata":{"execution":{"iopub.execute_input":"2022-07-29T06:59:27.044920Z","iopub.status.busy":"2022-07-29T06:59:27.044530Z","iopub.status.idle":"2022-07-29T06:59:27.051272Z","shell.execute_reply":"2022-07-29T06:59:27.050401Z","shell.execute_reply.started":"2022-07-29T06:59:27.044871Z"},"trusted":true}},{"cell_type":"code","source":"# Create a TransformerModel\nroberta_base_model = ClassificationModel(\n        \"xlmroberta\", \"xlm-roberta-base\",\n        num_labels=len(LABELS),\n        use_cuda=True,\n        args= {\n            \"overwrite_output_dir\": True,\n            \"num_train_epochs\": 30,\n            \"train_batch_size\":8,\n            \"learning_rate\": 1e-5,\n            # Use these parameters if you want to evaluate during training\n            \"evaluate_during_training\": True,\n            \"evaluate_during_training_steps\": steps_per_epoch*10,\n            \"evaluate_during_training_verbose\": True,\n            \"use_cached_eval_features\": True,\n            'reprocess_input_data': True,\n            \"labels_list\": LABELS,\n            # The following parameters are commented out because I want to save the model\n            \"no_cache\": True,\n            # Disable no_save: True if you want to save the model\n            \"no_save\": True,\n            \"max_seq_length\": 512,\n            \"save_steps\": -1,\n            # Only the trained model will be saved - to prevent filling all of the space\n            \"save_model_every_epoch\":False,\n            \"wandb_project\": 'X-GENRE classifiers',\n            \"silent\": True,\n            }\n        )","metadata":{"execution":{"iopub.status.busy":"2022-08-23T06:54:42.055130Z","iopub.execute_input":"2022-08-23T06:54:42.055527Z","iopub.status.idle":"2022-08-23T06:56:01.886261Z","shell.execute_reply.started":"2022-08-23T06:54:42.055494Z","shell.execute_reply":"2022-08-23T06:56:01.885082Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"# Train the model\nroberta_base_model.train_model(train_df, eval_df = dev_df)","metadata":{"execution":{"iopub.status.busy":"2022-08-23T06:56:01.888710Z","iopub.execute_input":"2022-08-23T06:56:01.889469Z","iopub.status.idle":"2022-08-23T07:19:28.680015Z","shell.execute_reply.started":"2022-08-23T06:56:01.889428Z","shell.execute_reply":"2022-08-23T07:19:28.678694Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"Based on evaluation during training, the optimum epoch is between 5 and 15 epochs.","metadata":{}},{"cell_type":"code","source":"# Create a file to save results into (you can find it under Data: Output). Be careful, run this step only once to not overwrite the results file.\nresults = []\n\nwith open(\"FTD-X-GENRE-Experiments-Results.json\", \"w\") as results_file:\n    json.dump(results,results_file, indent= \"\")","metadata":{"execution":{"iopub.status.busy":"2022-08-23T07:28:12.747010Z","iopub.execute_input":"2022-08-23T07:28:12.748153Z","iopub.status.idle":"2022-08-23T07:28:12.754730Z","shell.execute_reply.started":"2022-08-23T07:28:12.748113Z","shell.execute_reply":"2022-08-23T07:28:12.753634Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"# Open the main results file:\n\nprevious_results_file = open(\"FTD-X-GENRE-Experiments-Results.json\")\nprevious_results = json.load(previous_results_file)\nlen(previous_results)","metadata":{"execution":{"iopub.status.busy":"2022-08-23T07:28:20.635582Z","iopub.execute_input":"2022-08-23T07:28:20.636914Z","iopub.status.idle":"2022-08-23T07:28:20.650912Z","shell.execute_reply.started":"2022-08-23T07:28:20.636872Z","shell.execute_reply":"2022-08-23T07:28:20.649674Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"def testing(test_df, test_name, epoch):\n    \"\"\"\n    This function takes the test dataset and applies the trained model on it to infer predictions.\n    It also prints and saves a confusion matrix, calculates the F1 scores and saves the results in a list of results.\n\n    Args:\n    - test_df (pandas DataFrame)\n    - test_name\n    - epoch: num_train_epochs\n    \"\"\"\n    # Get the true labels\n    y_true = test_df.labels\n\n    model = roberta_base_model\n    \n    # Calculate the model's predictions on test\n    def make_prediction(input_string):\n        return model.predict([input_string])[0][0]\n\n    y_pred = test_df.text.apply(make_prediction)\n\n    # Calculate the scores\n    macro = f1_score(y_true, y_pred, labels=LABELS, average=\"macro\")\n    micro = f1_score(y_true, y_pred, labels=LABELS,  average=\"micro\")\n    print(f\"Macro f1: {macro:0.3}, Micro f1: {micro:0.3}\")\n\n    # Plot the confusion matrix:\n    cm = confusion_matrix(y_true, y_pred, labels=LABELS)\n    plt.figure(figsize=(9, 9))\n    plt.imshow(cm, cmap=\"Oranges\")\n    for (i, j), z in np.ndenumerate(cm):\n        plt.text(j, i, '{:d}'.format(z), ha='center', va='center')\n    classNames = LABELS\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n    tick_marks = np.arange(len(classNames))\n    plt.xticks(tick_marks, classNames, rotation=90)\n    plt.yticks(tick_marks, classNames)\n    plt.title(f\"{test_name}\")\n\n    plt.tight_layout()\n    fig1 = plt.gcf()\n    plt.show()\n    plt.draw()\n    fig1.savefig(f\"Confusion-matrix-{test_name}.png\",dpi=100)\n\n    # Save the results:\n    rezdict = {\n        \"experiment\": test_name,\n        \"num_train_epochs\": epoch,\n        \"train_batch_size\":8,\n        \"learning_rate\": 1e-5,\n        \"microF1\": micro,\n        \"macroF1\": macro,\n        \"y_true\": y_true.to_dict(),\n        \"y_pred\": y_pred.to_dict(),\n        }\n    previous_results.append(rezdict)\n\n    #Save intermediate results (just in case)\n    backup = []\n    backup.append(rezdict)\n    with open(f\"backup-results-{test_name}.json\", \"w\") as backup_file:\n        json.dump(backup,backup_file, indent= \"\")","metadata":{"execution":{"iopub.status.busy":"2022-08-23T07:28:42.106377Z","iopub.execute_input":"2022-08-23T07:28:42.106755Z","iopub.status.idle":"2022-08-23T07:28:42.119536Z","shell.execute_reply.started":"2022-08-23T07:28:42.106721Z","shell.execute_reply":"2022-08-23T07:28:42.118170Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"# Train the model for various epochs to find the optimum number\nepochs = [5, 8, 10, 13, 15]\n\nfor epoch in epochs:\n    roberta_base_model = ClassificationModel(\n                \"xlmroberta\", \"xlm-roberta-base\",\n                num_labels=len(LABELS),\n                use_cuda=True,\n                args= {\n                    \"overwrite_output_dir\": True,\n                    \"num_train_epochs\": epoch,\n                    \"train_batch_size\":8,\n                    \"learning_rate\": 1e-5,\n                    \"labels_list\": LABELS,\n                    # The following parameters (no_cache, no_save) are commented out if I want to save the model\n                    \"no_cache\": True,\n                    # Disable no_save: True if you want to save the model\n                    \"no_save\": True,\n                    \"max_seq_length\": 512,\n                    \"save_steps\": -1,\n                    # Only the trained model will be saved - to prevent filling all of the space\n                    \"save_model_every_epoch\":False,\n                    \"wandb_project\": 'X-GENRE classifiers',\n                    \"silent\": True,\n                    }\n                )\n\n    # Train the model\n    roberta_base_model.train_model(train_df)\n    \n    # Test the model on dev_df\n    testing(dev_df, f\"FTD-X-GENRE-dev-epoch-search:{epoch}\", epoch)","metadata":{"execution":{"iopub.status.busy":"2022-08-23T07:29:53.958624Z","iopub.execute_input":"2022-08-23T07:29:53.959554Z","iopub.status.idle":"2022-08-23T08:11:19.033042Z","shell.execute_reply.started":"2022-08-23T07:29:53.959505Z","shell.execute_reply":"2022-08-23T08:11:19.030659Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"Optimum number of epochs is 8.","metadata":{}},{"cell_type":"code","source":"# Compare the results by creating a dataframe from the previous_results dictionary:\nresults_df = pd.DataFrame(previous_results)\n\nresults_df","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Save the file with updated results.\nwith open(\"FTD-X-GENRE-Experiments-Results.json\", \"w\") as results_file:\n    json.dump(previous_results,results_file, indent= \"\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create a TransformerModel\nroberta_base_model = ClassificationModel(\n        \"xlmroberta\", \"xlm-roberta-base\",\n        num_labels=len(LABELS),\n        use_cuda=True,\n        args= {\n            \"overwrite_output_dir\": True,\n            \"num_train_epochs\": 8,\n            \"train_batch_size\":8,\n            \"learning_rate\": 1e-5,\n            \"labels_list\": LABELS,\n            # The following parameters are commented out because I want to save the model\n            #\"no_cache\": True,\n            # Disable no_save: True if you want to save the model\n            #\"no_save\": True,\n            \"max_seq_length\": 512,\n            \"save_steps\": -1,\n            # Only the trained model will be saved - to prevent filling all of the space\n            \"save_model_every_epoch\":False,\n            \"wandb_project\": 'X-GENRE classifiers',\n            \"silent\": True,\n            }\n        )","metadata":{"execution":{"iopub.status.busy":"2022-08-23T08:11:54.280579Z","iopub.execute_input":"2022-08-23T08:11:54.281664Z","iopub.status.idle":"2022-08-23T08:11:59.986110Z","shell.execute_reply.started":"2022-08-23T08:11:54.281589Z","shell.execute_reply":"2022-08-23T08:11:59.984916Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"# Train the model\nroberta_base_model.train_model(train_df)","metadata":{"execution":{"iopub.status.busy":"2022-08-23T08:11:59.988033Z","iopub.execute_input":"2022-08-23T08:11:59.988784Z","iopub.status.idle":"2022-08-23T08:17:25.044958Z","shell.execute_reply.started":"2022-08-23T08:11:59.988737Z","shell.execute_reply":"2022-08-23T08:17:25.043947Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"%ls /kaggle/working/outputs","metadata":{"execution":{"iopub.status.busy":"2022-08-23T08:25:56.356179Z","iopub.execute_input":"2022-08-23T08:25:56.356617Z","iopub.status.idle":"2022-08-23T08:25:57.524615Z","shell.execute_reply.started":"2022-08-23T08:25:56.356570Z","shell.execute_reply":"2022-08-23T08:25:57.523324Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"# Save the trained model to Wandb\nrun = wandb.init(project=\"X-GENRE classifiers\", entity=\"tajak\", name=\"saving-trained-model\")\ntrained_model_artifact = wandb.Artifact(\"FTD-X-GENRE-classifier\", type=\"model\", description=\"a model trained on the FTD dataset, annotated with X-GENRE labels.\")\ntrained_model_artifact.add_dir(\"/kaggle/working/outputs\")\nrun.log_artifact(trained_model_artifact)","metadata":{"execution":{"iopub.status.busy":"2022-08-23T08:26:34.618616Z","iopub.execute_input":"2022-08-23T08:26:34.619275Z","iopub.status.idle":"2022-08-23T08:26:50.339030Z","shell.execute_reply.started":"2022-08-23T08:26:34.619239Z","shell.execute_reply":"2022-08-23T08:26:50.337678Z"},"trusted":true},"execution_count":20,"outputs":[]}]}