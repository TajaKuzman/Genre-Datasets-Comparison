{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"Import all necessary libraries and install everything you need for training:","metadata":{}},{"cell_type":"code","source":"# install the libraries necessary for data wrangling, prediction and result analysis\nimport json\nimport numpy as np\nimport pandas as pd\nimport logging\nimport matplotlib.pyplot as plt\nfrom sklearn import metrics\nfrom sklearn.metrics import classification_report, confusion_matrix, f1_score,precision_score, recall_score\nimport torch\nfrom numba import cuda\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.dummy import DummyClassifier","metadata":{"execution":{"iopub.status.busy":"2022-07-29T06:57:33.176491Z","iopub.execute_input":"2022-07-29T06:57:33.176902Z","iopub.status.idle":"2022-07-29T06:57:35.902248Z","shell.execute_reply.started":"2022-07-29T06:57:33.176817Z","shell.execute_reply":"2022-07-29T06:57:35.901146Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"# Install transformers\n# (this needs to be done on Kaggle each time you start the session)\n!pip install -q transformers","metadata":{"execution":{"iopub.status.busy":"2022-07-29T06:57:35.907754Z","iopub.execute_input":"2022-07-29T06:57:35.908897Z","iopub.status.idle":"2022-07-29T06:57:47.363131Z","shell.execute_reply.started":"2022-07-29T06:57:35.908846Z","shell.execute_reply":"2022-07-29T06:57:47.361894Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"# Install the simpletransformers\n!pip install -q simpletransformers\nfrom simpletransformers.classification import ClassificationModel","metadata":{"execution":{"iopub.status.busy":"2022-07-29T06:57:47.365668Z","iopub.execute_input":"2022-07-29T06:57:47.366114Z","iopub.status.idle":"2022-07-29T06:58:15.436459Z","shell.execute_reply.started":"2022-07-29T06:57:47.366072Z","shell.execute_reply":"2022-07-29T06:58:15.435458Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"# Install wandb\n!pip install -q wandb","metadata":{"execution":{"iopub.status.busy":"2022-07-29T06:58:15.439205Z","iopub.execute_input":"2022-07-29T06:58:15.440070Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import wandb","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Login to wandb\nwandb.login()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Clean the GPU cache\n\ncuda.select_device(0)\ncuda.close()\ncuda.select_device(0)\ntorch.cuda.empty_cache()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Import the data","metadata":{}},{"cell_type":"code","source":"# FTD\ntrain_df = pd.read_csv(\"/kaggle/input/genredatasetscomparison/FTD-train.txt\", sep=\"\\t\", index_col=0)\ndev_df = pd.read_csv(\"/kaggle/input/genredatasetscomparison/FTD-dev.txt\", sep = \"\\t\", index_col = 0)\ntest_df = pd.read_csv(\"/kaggle/input/genredatasetscomparison/FTD-test.txt\", sep = \"\\t\", index_col = 0)\n\nprint(\"FTD train shape: {}, Dev shape: {}, Test shape: {}.\".format(train_df.shape, dev_df.shape, test_df.shape))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Training and testing","metadata":{}},{"cell_type":"markdown","source":"We will use the multilingual XLM-RoBERTa model\nhttps://huggingface.co/xlm-roberta-base","metadata":{}},{"cell_type":"code","source":"# Create a file to save results into (you can find it under Data: Output). Be careful, run this step only once to not overwrite the results file.\nresults = []\n\nwith open(\"FTD-Experiments-Results.json\", \"w\") as results_file:\n    json.dump(results,results_file, indent= \"\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Open the main results file:\n\nprevious_results_file = open(\"./FTD-Experiments-Results.json\")\nprevious_results = json.load(previous_results_file)\nlen(previous_results)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create a list of labels\nLABELS = train_df.labels.unique().tolist()\nLABELS","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Hyperparameter sweeps","metadata":{}},{"cell_type":"code","source":"# Configure the WandB sweep for hyperparameter search\nsweep_config = {\n    \"method\": \"grid\",  # random, grid, bayes\n    \"metric\": {\"name\": \"train_loss\", \"goal\": \"minimize\"},\n    \"parameters\": {\n        \"num_train_epochs\": {\"values\": [10, 20, 30, 50, 70, 90]},\n    },\n}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Initialize the sweep\nsweep_id = wandb.sweep(sweep_config, project=\"FTD-learning-hyperparameter-sweep\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Add logging\nlogging.basicConfig(level=logging.INFO)\ntransformers_logger = logging.getLogger(\"transformers\")\ntransformers_logger.setLevel(logging.WARNING)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create a function that will be ran inside the sweep\ndef train():\n    # Initialize a new wandb run\n    wandb.init()\n\n    # Create a TransformerModel\n    roberta_base_model = ClassificationModel(\n        \"xlmroberta\", \"xlm-roberta-base\",\n        num_labels=len(LABELS),\n        use_cuda=True,\n        args= {\n            \"overwrite_output_dir\": True,\n            \"labels_list\": LABELS,\n            \"no_cache\": True,\n            \"no_save\": True,\n            \"max_seq_length\": 512,\n            \"save_steps\": -1,\n            \"evaluate_during_training\":True,\n            'logging_steps': 10,\n            'evaluate_during_training_steps': 10,\n            \"use_cached_eval_features\": True,\n            \"reprocess_input_data\": True,\n            \"silent\": True,\n            \"wandb_project\": 'FTD-learning-hyperparameter-sweep',\n            \"sweep_config\":wandb.config\n            }\n        )\n\n    # Train the model\n    roberta_base_model.train_model(train_df, eval_df=dev_df)\n\n    # Evaluate the model\n    roberta_base_model.eval_model(dev_df)\n\n    # Sync wandb\n    wandb.join()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Run the sweeps\nwandb.agent(sweep_id, train, count= 10)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Training","metadata":{}},{"cell_type":"markdown","source":"I manually checked for the best parameters, experimenting only with the epochs by using evaluation during training. See the report in Wandb about it. The best epoch number revealed to be 10.","metadata":{}},{"cell_type":"code","source":"# Initialize Wandb\nwandb.init(project=\"FTD-learning-manual-hyperparameter-search\", entity=\"tajak\", name=\"saving-trained-model\")","metadata":{"execution":{"iopub.status.busy":"2022-07-29T06:38:07.245031Z","iopub.execute_input":"2022-07-29T06:38:07.245965Z","iopub.status.idle":"2022-07-29T06:38:09.966411Z","shell.execute_reply.started":"2022-07-29T06:38:07.245914Z","shell.execute_reply":"2022-07-29T06:38:09.965505Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"# Import LOGGING - makes the Transformer logger less verbose\nimport logging\n\nlogging.basicConfig(level=logging.INFO)\n\n# Get root logger (all other loggers will be derived from this logger's\n# properties)\nlogger = logging.getLogger()\nlogger.warning(\"Is this working?\") \n\n# Get the logger for the huggingface/transformers library.\ntransformers_logger = logging.getLogger(\"transformers\")\n\n# Set the logging level to warning, meaning display warnings and worse, but \n# don't display any `INFO` logs.\ntransformers_logger.setLevel(logging.WARNING)","metadata":{"execution":{"iopub.status.busy":"2022-07-29T06:38:09.968850Z","iopub.execute_input":"2022-07-29T06:38:09.969550Z","iopub.status.idle":"2022-07-29T06:38:09.977652Z","shell.execute_reply.started":"2022-07-29T06:38:09.969512Z","shell.execute_reply":"2022-07-29T06:38:09.976666Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"# Calculate how many steps will each epoch have\n# Num steps in epoch = training samples / batch size\nsteps_per_epoch = int(849/8)\nsteps_per_epoch","metadata":{"execution":{"iopub.status.busy":"2022-07-29T06:38:09.979167Z","iopub.execute_input":"2022-07-29T06:38:09.979761Z","iopub.status.idle":"2022-07-29T06:38:09.990685Z","shell.execute_reply.started":"2022-07-29T06:38:09.979726Z","shell.execute_reply":"2022-07-29T06:38:09.989411Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"# I'll evaluate per every 10th epoch - per 1060 steps.","metadata":{"execution":{"iopub.status.busy":"2022-07-29T06:38:09.993695Z","iopub.execute_input":"2022-07-29T06:38:09.994057Z","iopub.status.idle":"2022-07-29T06:38:10.000810Z","shell.execute_reply.started":"2022-07-29T06:38:09.994022Z","shell.execute_reply":"2022-07-29T06:38:09.999937Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"# Create a TransformerModel\nroberta_base_model = ClassificationModel(\n        \"xlmroberta\", \"xlm-roberta-base\",\n        num_labels=len(LABELS),\n        use_cuda=True,\n        args= {\n            \"overwrite_output_dir\": True,\n            \"num_train_epochs\": 10,\n            \"train_batch_size\":8,\n            \"learning_rate\": 1e-5,\n            \"evaluate_during_training\": True,\n            \"evaluate_during_training_steps\": steps_per_epoch*10,\n            \"evaluate_during_training_verbose\": True,\n            \"use_cached_eval_features\": True,\n            'reprocess_input_data': True,\n            \"labels_list\": LABELS,\n            \"no_cache\": True,\n            # Disable no_save: True if you want to save the model\n            #\"no_save\": True,\n            \"max_seq_length\": 512,\n            \"save_steps\": -1,\n            \"wandb_project\": 'FTD-learning-manual-hyperparameter-search',\n            }\n        )","metadata":{"execution":{"iopub.status.busy":"2022-07-29T06:53:36.811982Z","iopub.execute_input":"2022-07-29T06:53:36.812358Z","iopub.status.idle":"2022-07-29T06:54:11.250998Z","shell.execute_reply.started":"2022-07-29T06:53:36.812327Z","shell.execute_reply":"2022-07-29T06:54:11.249449Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"# Train the model\nroberta_base_model.train_model(train_df)","metadata":{"execution":{"iopub.status.busy":"2022-07-29T06:45:21.026687Z","iopub.execute_input":"2022-07-29T06:45:21.027454Z","iopub.status.idle":"2022-07-29T06:52:58.370853Z","shell.execute_reply.started":"2022-07-29T06:45:21.027409Z","shell.execute_reply":"2022-07-29T06:52:58.369418Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"# See where the model is saved - check if the same things are in outputs - maybe there is the final model (check model_args)\n!ls /kaggle/working/outputs/best_model","metadata":{"execution":{"iopub.status.busy":"2022-07-29T06:38:10.576430Z","iopub.status.idle":"2022-07-29T06:38:10.576844Z","shell.execute_reply.started":"2022-07-29T06:38:10.576626Z","shell.execute_reply":"2022-07-29T06:38:10.576647Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Save the trained model to Wandb\ntrained_model_artifact = wandb.Artifact(\"FTD-classifier\", type=\"model\", description=\"a model trained on the FTD dataset\")\ntrained_model_artifact.add_dir(\"/kaggle/working/outputs/best_model\")\nrun.log_artifact(trained_model_artifact)","metadata":{"execution":{"iopub.status.busy":"2022-07-29T06:38:10.577843Z","iopub.status.idle":"2022-07-29T06:38:10.578272Z","shell.execute_reply.started":"2022-07-29T06:38:10.578056Z","shell.execute_reply":"2022-07-29T06:38:10.578077Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# To load the model from Wandb\nmodel_name = \"FTD-classifier\"\n# Use the latest version of the model\nmodel_at = run.use_artifact(model_name + \":lastest\")\n# Download the directory\nmodel_dir = model.at_download()\n\n# Loading a local save\nmodel = ClassificationModel(\n    \"xlmroberta\", model_dir)","metadata":{"execution":{"iopub.status.busy":"2022-07-29T06:38:10.579220Z","iopub.status.idle":"2022-07-29T06:38:10.579621Z","shell.execute_reply.started":"2022-07-29T06:38:10.579409Z","shell.execute_reply":"2022-07-29T06:38:10.579429Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Loading a local save\nmodel = ClassificationModel(\n    \"xlmroberta\", \"/kaggle/working/outputs/best_model\")","metadata":{"execution":{"iopub.status.busy":"2022-07-29T06:38:10.580550Z","iopub.status.idle":"2022-07-29T06:38:10.580972Z","shell.execute_reply.started":"2022-07-29T06:38:10.580738Z","shell.execute_reply":"2022-07-29T06:38:10.580757Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def testing(test_df, test_name):\n    \"\"\"\n    This function takes the test dataset and applies the trained model on it to infer predictions.\n    It also prints and saves a confusion matrix, calculates the F1 scores and saves the results in a list of results.\n\n    Args:\n    - test_df (pandas DataFrame)\n    - test_name\n    \"\"\"\n    # Get the true labels\n    y_true = test_df.labels\n    \n    model = roberta_base_model\n\n    # Calculate the model's predictions on test\n    def make_prediction(input_string):\n        return model.predict([input_string])[0][0]\n\n    y_pred = test_df.text.apply(make_prediction)\n\n    # Calculate the scores\n    macro = f1_score(y_true, y_pred, labels=LABELS, average=\"macro\")\n    micro = f1_score(y_true, y_pred, labels=LABELS,  average=\"micro\")\n    print(f\"Macro f1: {macro:0.3}, Micro f1: {micro:0.3}\")\n\n    # Plot the confusion matrix:\n    cm = confusion_matrix(y_true, y_pred, labels=LABELS)\n    plt.figure(figsize=(9, 9))\n    plt.imshow(cm, cmap=\"Oranges\")\n    for (i, j), z in np.ndenumerate(cm):\n        plt.text(j, i, '{:d}'.format(z), ha='center', va='center')\n    classNames = LABELS\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n    tick_marks = np.arange(len(classNames))\n    plt.xticks(tick_marks, classNames, rotation=90)\n    plt.yticks(tick_marks, classNames)\n    plt.title(f\"{test_name}\")\n\n    plt.tight_layout()\n    fig1 = plt.gcf()\n    plt.show()\n    plt.draw()\n    fig1.savefig(f\"Confusion-matrix-{test_name}.png\",dpi=100)\n\n    # Save the results:\n    rezdict = {\n        \"experiment\": test_name,\n        \"num_train_epochs\": 10,\n        \"train_batch_size\":8,\n        \"learning_rate\": 1e-5,\n        \"microF1\": micro,\n        \"macroF1\": macro,\n        \"y_true\": y_true.tolist(),\n        \"y_pred\": y_pred.tolist(),\n        }\n    previous_results.append(rezdict)\n\n    #Save intermediate results (just in case)\n    backup = []\n    backup.append(rezdict)\n    with open(f\"backup-results-{test_name}.json\", \"w\") as backup_file:\n        json.dump(backup,backup_file, indent= \"\")","metadata":{"execution":{"iopub.status.busy":"2022-07-29T06:38:10.581957Z","iopub.status.idle":"2022-07-29T06:38:10.582359Z","shell.execute_reply.started":"2022-07-29T06:38:10.582150Z","shell.execute_reply":"2022-07-29T06:38:10.582169Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"testing(dev_df, \"hyperparameter-search-final-evaluation-10-epochs\")","metadata":{"execution":{"iopub.status.busy":"2022-07-29T06:38:10.583290Z","iopub.status.idle":"2022-07-29T06:38:10.583686Z","shell.execute_reply.started":"2022-07-29T06:38:10.583473Z","shell.execute_reply":"2022-07-29T06:38:10.583492Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"testing(test_df, \"FTD-test-10-epochs\")","metadata":{"execution":{"iopub.status.busy":"2022-07-29T06:38:10.584616Z","iopub.status.idle":"2022-07-29T06:38:10.585035Z","shell.execute_reply.started":"2022-07-29T06:38:10.584801Z","shell.execute_reply":"2022-07-29T06:38:10.584820Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"previous_results = previous_results[:2]","metadata":{"execution":{"iopub.status.busy":"2022-07-29T06:38:10.585976Z","iopub.status.idle":"2022-07-29T06:38:10.586455Z","shell.execute_reply.started":"2022-07-29T06:38:10.586208Z","shell.execute_reply":"2022-07-29T06:38:10.586231Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Compare the results by creating a dataframe from the previous_results dictionary:\nresults_df = pd.DataFrame(previous_results)\n\nresults_df","metadata":{"execution":{"iopub.status.busy":"2022-07-29T06:38:10.593953Z","iopub.status.idle":"2022-07-29T06:38:10.594429Z","shell.execute_reply.started":"2022-07-29T06:38:10.594186Z","shell.execute_reply":"2022-07-29T06:38:10.594209Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Save the file with updated results.\nwith open(\"FTD-Experiments-Results.json\", \"w\") as results_file:\n    json.dump(previous_results,results_file, indent= \"\")","metadata":{"execution":{"iopub.status.busy":"2022-07-29T06:38:10.604497Z","iopub.status.idle":"2022-07-29T06:38:10.605059Z","shell.execute_reply.started":"2022-07-29T06:38:10.604754Z","shell.execute_reply":"2022-07-29T06:38:10.604783Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!ls /kaggle/working/runs/Jul28_10-01-31_228df68315e6/events.out.tfevents.1659002491.228df68315e6.33.0","metadata":{"execution":{"iopub.status.busy":"2022-07-29T06:38:10.606830Z","iopub.status.idle":"2022-07-29T06:38:10.608181Z","shell.execute_reply.started":"2022-07-29T06:38:10.607767Z","shell.execute_reply":"2022-07-29T06:38:10.607791Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Save the model to wandb\n# \"model.h5\" is saved in wandb.run.dir & will be uploaded at the end of training\nroberta_base_model.save(os.path.join(wandb.run.dir, \"model.h5\"))\n\n# Save a model file manually from the current directory\nroberta_base_model.save('model.h5')\n\n# Save all files that currently exist containing the substring \"ckpt\":\nroberta_base_model.save('../logs/*ckpt*')\n\n# Save any files starting with \"checkpoint\" as they're written to:\nroberta_base_model.save(os.path.join(wandb.run.dir, \"checkpoint*\"))","metadata":{"execution":{"iopub.status.busy":"2022-07-29T06:38:10.609622Z","iopub.status.idle":"2022-07-29T06:38:10.610953Z","shell.execute_reply.started":"2022-07-29T06:38:10.610574Z","shell.execute_reply":"2022-07-29T06:38:10.610598Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Clean the GPU cache\ncuda.select_device(0)\ncuda.close()\ncuda.select_device(0)\ntorch.cuda.empty_cache()","metadata":{"execution":{"iopub.status.busy":"2022-07-29T06:38:10.612437Z","iopub.status.idle":"2022-07-29T06:38:10.613804Z","shell.execute_reply.started":"2022-07-29T06:38:10.613423Z","shell.execute_reply":"2022-07-29T06:38:10.613448Z"},"trusted":true},"execution_count":null,"outputs":[]}]}