{"cells":[{"cell_type":"markdown","metadata":{},"source":["Import all necessary libraries and install everything you need for training:"]},{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2022-08-04T11:59:41.352999Z","iopub.status.busy":"2022-08-04T11:59:41.352276Z","iopub.status.idle":"2022-08-04T11:59:44.149064Z","shell.execute_reply":"2022-08-04T11:59:44.147972Z","shell.execute_reply.started":"2022-08-04T11:59:41.352863Z"},"trusted":true},"outputs":[],"source":["# install the libraries necessary for data wrangling, prediction and result analysis\n","import json\n","import numpy as np\n","import pandas as pd\n","import logging\n","import matplotlib.pyplot as plt\n","from sklearn import metrics\n","from sklearn.metrics import classification_report, confusion_matrix, f1_score,precision_score, recall_score\n","import torch\n","from numba import cuda\n","from sklearn.model_selection import train_test_split\n","from sklearn.dummy import DummyClassifier"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2022-08-04T11:59:47.475660Z","iopub.status.busy":"2022-08-04T11:59:47.474343Z","iopub.status.idle":"2022-08-04T12:00:25.907493Z","shell.execute_reply":"2022-08-04T12:00:25.906466Z","shell.execute_reply.started":"2022-08-04T11:59:47.475611Z"},"trusted":true},"outputs":[],"source":["# Install transformers\n","# (this needs to be done on Kaggle each time you start the session)\n","!pip install -q transformers\n","\n","# Install the simpletransformers\n","!pip install -q simpletransformers\n","from simpletransformers.classification import ClassificationModel"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2022-08-04T12:00:25.910560Z","iopub.status.busy":"2022-08-04T12:00:25.909717Z","iopub.status.idle":"2022-08-04T12:00:58.154171Z","shell.execute_reply":"2022-08-04T12:00:58.152915Z","shell.execute_reply.started":"2022-08-04T12:00:25.910522Z"},"trusted":true},"outputs":[],"source":["# Install wandb\n","!pip install -q wandb\n","\n","import wandb\n","\n","# Login to wandb\n","wandb.login()"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2022-08-04T12:01:00.622733Z","iopub.status.busy":"2022-08-04T12:01:00.619624Z","iopub.status.idle":"2022-08-04T12:01:01.013543Z","shell.execute_reply":"2022-08-04T12:01:01.012473Z","shell.execute_reply.started":"2022-08-04T12:01:00.622682Z"},"trusted":true},"outputs":[],"source":["# Clean the GPU cache\n","\n","cuda.select_device(0)\n","cuda.close()\n","cuda.select_device(0)\n","torch.cuda.empty_cache()\n"]},{"cell_type":"markdown","metadata":{},"source":["### Import the data"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2022-08-04T12:01:06.269780Z","iopub.status.busy":"2022-08-04T12:01:06.269237Z","iopub.status.idle":"2022-08-04T12:01:08.307902Z","shell.execute_reply":"2022-08-04T12:01:08.306843Z","shell.execute_reply.started":"2022-08-04T12:01:06.269737Z"},"trusted":true},"outputs":[],"source":["# CORE-main (texts annotated with main CORE labels)\n","train_df = pd.read_csv(\"/kaggle/input/genredatasetscomparison/CORE-main-subset-train.csv\", index_col = 0)\n","dev_df = pd.read_csv(\"/kaggle/input/genredatasetscomparison/CORE-main-subset-dev-sample.csv\", index_col = 0)\n","test_df = pd.read_csv(\"/kaggle/input/genredatasetscomparison/CORE-main-subset-test.csv\", index_col = 0)\n","\n","print(\"CORE-main train shape: {}, Dev shape: {}, Test shape: {}.\".format(train_df.shape, dev_df.shape, test_df.shape))"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2022-08-04T12:01:08.311605Z","iopub.status.busy":"2022-08-04T12:01:08.310647Z","iopub.status.idle":"2022-08-04T12:01:08.326175Z","shell.execute_reply":"2022-08-04T12:01:08.324973Z","shell.execute_reply.started":"2022-08-04T12:01:08.311568Z"},"trusted":true},"outputs":[],"source":["train_df.head()"]},{"cell_type":"markdown","metadata":{},"source":["## Training and saving"]},{"cell_type":"markdown","metadata":{},"source":["We will use the multilingual XLM-RoBERTa model\n","https://huggingface.co/xlm-roberta-base"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2022-08-04T12:01:16.124944Z","iopub.status.busy":"2022-08-04T12:01:16.124410Z","iopub.status.idle":"2022-08-04T12:01:16.134385Z","shell.execute_reply":"2022-08-04T12:01:16.133176Z","shell.execute_reply.started":"2022-08-04T12:01:16.124904Z"},"trusted":true},"outputs":[],"source":["import os\n","os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2022-08-04T12:01:17.931921Z","iopub.status.busy":"2022-08-04T12:01:17.931562Z","iopub.status.idle":"2022-08-04T12:01:17.942199Z","shell.execute_reply":"2022-08-04T12:01:17.940991Z","shell.execute_reply.started":"2022-08-04T12:01:17.931891Z"},"trusted":true},"outputs":[],"source":["# Create a list of labels\n","LABELS = train_df.labels.unique().tolist()\n","LABELS"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2022-08-04T12:01:20.925552Z","iopub.status.busy":"2022-08-04T12:01:20.924854Z","iopub.status.idle":"2022-08-04T12:01:23.693431Z","shell.execute_reply":"2022-08-04T12:01:23.692334Z","shell.execute_reply.started":"2022-08-04T12:01:20.925517Z"},"trusted":true},"outputs":[],"source":["# Initialize Wandb\n","wandb.init(project=\"CORE-hyperparameter-search\", name=\"training-the-model\")"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2022-08-04T12:01:25.215526Z","iopub.status.busy":"2022-08-04T12:01:25.215000Z","iopub.status.idle":"2022-08-04T12:01:25.224252Z","shell.execute_reply":"2022-08-04T12:01:25.223245Z","shell.execute_reply.started":"2022-08-04T12:01:25.215491Z"},"trusted":true},"outputs":[],"source":["# Calculate how many steps will each epoch have\n","# Num steps in epoch = training samples / batch size\n","steps_per_epoch = int(10256/8)\n","steps_per_epoch"]},{"cell_type":"markdown","metadata":{"execution":{"iopub.execute_input":"2022-07-29T06:59:27.044920Z","iopub.status.busy":"2022-07-29T06:59:27.044530Z","iopub.status.idle":"2022-07-29T06:59:27.051272Z","shell.execute_reply":"2022-07-29T06:59:27.050401Z","shell.execute_reply.started":"2022-07-29T06:59:27.044871Z"},"trusted":true},"source":["I evaluated per every 5th epoch - per 6410 steps. I first trained the model while evaluating it to find the optimal number of epochs. As previous experiments on smaller datasets showed that already 10-20 epochs are enough, we will here try up to 20 epochs, since this dataset is much bigger."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-08-03T13:07:34.281807Z","iopub.status.busy":"2022-08-03T13:07:34.280932Z","iopub.status.idle":"2022-08-03T17:14:30.791414Z","shell.execute_reply":"2022-08-03T17:14:30.788861Z","shell.execute_reply.started":"2022-08-03T13:07:34.281770Z"},"trusted":true},"outputs":[],"source":["# Create a TransformerModel and evaluate during training\n","epoch = 20\n","\n","roberta_base_model = ClassificationModel(\n","        \"xlmroberta\", \"xlm-roberta-base\",\n","        num_labels=len(LABELS),\n","        use_cuda=True,\n","        args= {\n","            \"overwrite_output_dir\": True,\n","            \"num_train_epochs\": epoch,\n","            \"train_batch_size\":8,\n","            \"learning_rate\": 1e-5,\n","            # Use these parameters if you want to evaluate during training\n","            \"evaluate_during_training\": True,\n","            \"evaluate_during_training_steps\": steps_per_epoch*5,\n","            \"evaluate_during_training_verbose\": True,\n","            \"use_cached_eval_features\": True,\n","            'reprocess_input_data': True,\n","            \"labels_list\": LABELS,\n","            # The following parameters (no_cache, no_save) are commented out if I want to save the model\n","            \"no_cache\": True,\n","            # Disable no_save: True if you want to save the model\n","            \"no_save\": True,\n","            \"max_seq_length\": 512,\n","            \"save_steps\": -1,\n","            # Only the trained model will be saved - to prevent filling all of the space\n","            \"save_model_every_epoch\":False,\n","            \"wandb_project\": 'CORE-hyperparameter-search',\n","            \"silent\": True,\n","            }\n","        )\n","\n","# Train the model and evaluate during training\n","roberta_base_model.train_model(train_df, eval_df = dev_df)"]},{"cell_type":"markdown","metadata":{},"source":["Evaluation during training showed that the number of epochs before the eval_loss starts rising is somewhere between epochs 2 and 6. We then trained the model for epochs 2, 4, 6 and 8 to find the optimum number."]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2022-08-04T06:18:45.097993Z","iopub.status.busy":"2022-08-04T06:18:45.097208Z","iopub.status.idle":"2022-08-04T06:18:45.107921Z","shell.execute_reply":"2022-08-04T06:18:45.106646Z","shell.execute_reply.started":"2022-08-04T06:18:45.097954Z"},"trusted":true},"outputs":[],"source":["# Create a file to save results into (you can find it under Data: Output). Be careful, run this step only once to not overwrite the results file.\n","results = []\n","\n","with open(\"CORE-main-Experiments-Results.json\", \"w\") as results_file:\n","    json.dump(results,results_file, indent= \"\")"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2022-08-04T06:19:02.694449Z","iopub.status.busy":"2022-08-04T06:19:02.693860Z","iopub.status.idle":"2022-08-04T06:19:02.704740Z","shell.execute_reply":"2022-08-04T06:19:02.703502Z","shell.execute_reply.started":"2022-08-04T06:19:02.694414Z"},"trusted":true},"outputs":[],"source":["# Open the main results file:\n","\n","previous_results_file = open(\"CORE-main-Experiments-Results.json\")\n","previous_results = json.load(previous_results_file)\n","len(previous_results)"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2022-08-04T12:01:49.364875Z","iopub.status.busy":"2022-08-04T12:01:49.364283Z","iopub.status.idle":"2022-08-04T12:01:49.378286Z","shell.execute_reply":"2022-08-04T12:01:49.377238Z","shell.execute_reply.started":"2022-08-04T12:01:49.364837Z"},"trusted":true},"outputs":[],"source":["def testing(test_df, test_name, epoch):\n","    \"\"\"\n","    This function takes the test dataset and applies the trained model on it to infer predictions.\n","    It also prints and saves a confusion matrix, calculates the F1 scores and saves the results in a list of results.\n","\n","    Args:\n","    - test_df (pandas DataFrame)\n","    - test_name\n","    - epoch: num_train_epochs\n","    \"\"\"\n","    # Get the true labels\n","    y_true = test_df.labels\n","\n","    model = roberta_base_model\n","    \n","    # Calculate the model's predictions on test\n","    def make_prediction(input_string):\n","        return model.predict([input_string])[0][0]\n","\n","    y_pred = test_df.text.apply(make_prediction)\n","\n","    # Calculate the scores\n","    macro = f1_score(y_true, y_pred, labels=LABELS, average=\"macro\")\n","    micro = f1_score(y_true, y_pred, labels=LABELS,  average=\"micro\")\n","    print(f\"Macro f1: {macro:0.3}, Micro f1: {micro:0.3}\")\n","\n","    # Plot the confusion matrix:\n","    cm = confusion_matrix(y_true, y_pred, labels=LABELS)\n","    plt.figure(figsize=(9, 9))\n","    plt.imshow(cm, cmap=\"Oranges\")\n","    for (i, j), z in np.ndenumerate(cm):\n","        plt.text(j, i, '{:d}'.format(z), ha='center', va='center')\n","    classNames = LABELS\n","    plt.ylabel('True label')\n","    plt.xlabel('Predicted label')\n","    tick_marks = np.arange(len(classNames))\n","    plt.xticks(tick_marks, classNames, rotation=90)\n","    plt.yticks(tick_marks, classNames)\n","    plt.title(f\"{test_name}\")\n","\n","    plt.tight_layout()\n","    fig1 = plt.gcf()\n","    plt.show()\n","    plt.draw()\n","    fig1.savefig(f\"Confusion-matrix-{test_name}.png\",dpi=100)\n","\n","    # Save the results:\n","    rezdict = {\n","        \"experiment\": test_name,\n","        \"num_train_epochs\": epoch,\n","        \"train_batch_size\":8,\n","        \"learning_rate\": 1e-5,\n","        \"microF1\": micro,\n","        \"macroF1\": macro,\n","        \"y_true\": y_true.to_dict(),\n","        \"y_pred\": y_pred.to_dict(),\n","        }\n","    previous_results.append(rezdict)\n","\n","    #Save intermediate results (just in case)\n","    backup = []\n","    backup.append(rezdict)\n","    with open(f\"backup-results-{test_name}.json\", \"w\") as backup_file:\n","        json.dump(backup,backup_file, indent= \"\")"]},{"cell_type":"markdown","metadata":{},"source":["2 epochs: 2564 steps, 4 epochs: 5128 steps, 6 epochs: 7692 steps, 8 epochs: 10.256 steps"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-08-04T06:20:01.525208Z","iopub.status.busy":"2022-08-04T06:20:01.524570Z"},"trusted":true},"outputs":[],"source":["# Train the model for various epochs to find the optimum number\n","epochs = [2, 4, 6, 8]\n","\n","for epoch in epochs:\n","    roberta_base_model = ClassificationModel(\n","                \"xlmroberta\", \"xlm-roberta-base\",\n","                num_labels=len(LABELS),\n","                use_cuda=True,\n","                args= {\n","                    \"overwrite_output_dir\": True,\n","                    \"num_train_epochs\": epoch,\n","                    \"train_batch_size\":8,\n","                    \"learning_rate\": 1e-5,\n","                    \"labels_list\": LABELS,\n","                    # The following parameters (no_cache, no_save) are commented out if I want to save the model\n","                    \"no_cache\": True,\n","                    # Disable no_save: True if you want to save the model\n","                    \"no_save\": True,\n","                    \"max_seq_length\": 512,\n","                    \"save_steps\": -1,\n","                    # Only the trained model will be saved - to prevent filling all of the space\n","                    \"save_model_every_epoch\":False,\n","                    \"wandb_project\": 'CORE-hyperparameter-search',\n","                    \"silent\": True,\n","                    }\n","                )\n","\n","    # Train the model\n","    roberta_base_model.train_model(train_df)\n","    \n","    # Test the model on dev_df\n","    testing(dev_df, f\"CORE-main-dev-epoch-search:{epoch}\", epoch)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Compare the results by creating a dataframe from the previous_results dictionary:\n","results_df = pd.DataFrame(previous_results)\n","\n","results_df"]},{"cell_type":"markdown","metadata":{},"source":["The experiments revealed that the optimum number of epochs is 4. The run crashed, so the results were not saved, but the results for epochs were the following: poech 2: Macro f1: 0.52, Micro f1: 0.71; epoch 4: Macro f1: 0.621, Micro f1: 0.733; epoch 6: Macro f1: 0.626, Micro f1: 0.724. "]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Save the file with updated results.\n","with open(\"results/CORE-main-Experiments-Results.json\", \"w\") as results_file:\n","    json.dump(previous_results,results_file, indent= \"\")"]},{"cell_type":"markdown","metadata":{},"source":["Train the model and save it."]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2022-08-04T12:09:20.930253Z","iopub.status.busy":"2022-08-04T12:09:20.929288Z","iopub.status.idle":"2022-08-04T12:10:17.473967Z","shell.execute_reply":"2022-08-04T12:10:17.472933Z","shell.execute_reply.started":"2022-08-04T12:09:20.930215Z"},"trusted":true},"outputs":[],"source":["# Create a TransformerModel\n","roberta_base_model = ClassificationModel(\n","        \"xlmroberta\", \"xlm-roberta-base\",\n","        num_labels=len(LABELS),\n","        use_cuda=True,\n","        args= {\n","            \"overwrite_output_dir\": True,\n","            \"num_train_epochs\": 4,\n","            \"train_batch_size\":8,\n","            \"learning_rate\": 1e-5,\n","            \"labels_list\": LABELS,\n","            # The following parameters are commented out because I want to save the model\n","            #\"no_cache\": True,\n","            # Disable no_save: True if you want to save the model\n","            #\"no_save\": True,\n","            \"max_seq_length\": 512,\n","            \"save_steps\": -1,\n","            # Only the trained model will be saved - to prevent filling all of the space\n","            \"save_model_every_epoch\":False,\n","            \"wandb_project\": 'CORE-hyperparameter-search',\n","            \"silent\": True,\n","            }\n","        )"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2022-08-04T12:16:09.993507Z","iopub.status.busy":"2022-08-04T12:16:09.993105Z","iopub.status.idle":"2022-08-04T12:59:33.362569Z","shell.execute_reply":"2022-08-04T12:59:33.361490Z","shell.execute_reply.started":"2022-08-04T12:16:09.993472Z"},"trusted":true},"outputs":[],"source":["# Train the model\n","roberta_base_model.train_model(train_df)"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2022-08-04T13:00:28.459005Z","iopub.status.busy":"2022-08-04T13:00:28.458378Z","iopub.status.idle":"2022-08-04T13:00:29.581572Z","shell.execute_reply":"2022-08-04T13:00:29.580343Z","shell.execute_reply.started":"2022-08-04T13:00:28.458970Z"},"trusted":true},"outputs":[],"source":["!ls /kaggle/working/outputs"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2022-08-04T13:01:09.719476Z","iopub.status.busy":"2022-08-04T13:01:09.718163Z","iopub.status.idle":"2022-08-04T13:01:25.401387Z","shell.execute_reply":"2022-08-04T13:01:25.400457Z","shell.execute_reply.started":"2022-08-04T13:01:09.719414Z"},"trusted":true},"outputs":[],"source":["# Save the trained model to Wandb\n","run = wandb.init(project=\"CORE-hyperparameter-search\", entity=\"tajak\", name=\"saving-trained-model\")\n","trained_model_artifact = wandb.Artifact(\"CORE-main-classifier\", type=\"model\", description=\"a model trained on the CORE dataset with the main categories as labels. There are 9 labels: 'Interactive Discussion', 'Narrative', 'Informational Description/Explanation','Lyrical', 'Opinion', 'Informational Persuasion','Spoken','How-To/Instructional', 'Other'.\")\n","trained_model_artifact.add_dir(\"/kaggle/working/outputs\")\n","run.log_artifact(trained_model_artifact)"]}],"metadata":{"kernelspec":{"display_name":"Python 3.9.7 ('base')","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.7"},"vscode":{"interpreter":{"hash":"7e373e41fe05b496006fe2fc132d7af19f1d513370c44925a0044a5f3ee41336"}}},"nbformat":4,"nbformat_minor":4}
