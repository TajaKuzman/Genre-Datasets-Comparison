{"cells":[{"cell_type":"markdown","metadata":{},"source":["Import all necessary libraries and install everything you need for training:"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2022-07-29T06:57:33.176902Z","iopub.status.busy":"2022-07-29T06:57:33.176491Z","iopub.status.idle":"2022-07-29T06:57:35.902248Z","shell.execute_reply":"2022-07-29T06:57:35.901146Z","shell.execute_reply.started":"2022-07-29T06:57:33.176817Z"},"trusted":true},"outputs":[],"source":["# install the libraries necessary for data wrangling, prediction and result analysis\n","import json\n","import numpy as np\n","import pandas as pd\n","import logging\n","import matplotlib.pyplot as plt\n","from sklearn import metrics\n","from sklearn.metrics import classification_report, confusion_matrix, f1_score,precision_score, recall_score\n","import torch\n","from numba import cuda\n","from sklearn.model_selection import train_test_split\n","from sklearn.dummy import DummyClassifier"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2022-07-29T06:57:35.908897Z","iopub.status.busy":"2022-07-29T06:57:35.907754Z","iopub.status.idle":"2022-07-29T06:57:47.363131Z","shell.execute_reply":"2022-07-29T06:57:47.361894Z","shell.execute_reply.started":"2022-07-29T06:57:35.908846Z"},"trusted":true},"outputs":[],"source":["# Install transformers\n","# (this needs to be done on Kaggle each time you start the session)\n","#!pip install -q transformers"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2022-07-29T06:57:47.366114Z","iopub.status.busy":"2022-07-29T06:57:47.365668Z","iopub.status.idle":"2022-07-29T06:58:15.436459Z","shell.execute_reply":"2022-07-29T06:58:15.435458Z","shell.execute_reply.started":"2022-07-29T06:57:47.366072Z"},"trusted":true},"outputs":[],"source":["# Install the simpletransformers\n","#!pip install -q simpletransformers\n","from simpletransformers.classification import ClassificationModel"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2022-07-29T06:58:15.440070Z","iopub.status.busy":"2022-07-29T06:58:15.439205Z","iopub.status.idle":"2022-07-29T06:58:25.453542Z","shell.execute_reply":"2022-07-29T06:58:25.452416Z","shell.execute_reply.started":"2022-07-29T06:58:15.440031Z"},"trusted":true},"outputs":[],"source":["# Install wandb\n","#!pip install -q wandb"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2022-07-29T06:58:25.456001Z","iopub.status.busy":"2022-07-29T06:58:25.455601Z","iopub.status.idle":"2022-07-29T06:58:25.462969Z","shell.execute_reply":"2022-07-29T06:58:25.462054Z","shell.execute_reply.started":"2022-07-29T06:58:25.455962Z"},"trusted":true},"outputs":[],"source":["import wandb"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2022-07-29T06:58:25.464749Z","iopub.status.busy":"2022-07-29T06:58:25.464432Z","iopub.status.idle":"2022-07-29T06:58:57.622701Z","shell.execute_reply":"2022-07-29T06:58:57.621772Z","shell.execute_reply.started":"2022-07-29T06:58:25.464717Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mtajak\u001b[0m (use `wandb login --relogin` to force relogin)\n"]},{"data":{"text/plain":["True"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["# Login to wandb\n","wandb.login()"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2022-07-29T06:58:57.624647Z","iopub.status.busy":"2022-07-29T06:58:57.624034Z","iopub.status.idle":"2022-07-29T06:58:57.929939Z","shell.execute_reply":"2022-07-29T06:58:57.928897Z","shell.execute_reply.started":"2022-07-29T06:58:57.624608Z"},"trusted":true},"outputs":[],"source":["# Clean the GPU cache\n","\n","cuda.select_device(0)\n","cuda.close()\n","cuda.select_device(0)\n","torch.cuda.empty_cache()\n"]},{"cell_type":"markdown","metadata":{},"source":["### Import the data"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2022-07-29T06:58:57.932449Z","iopub.status.busy":"2022-07-29T06:58:57.931401Z","iopub.status.idle":"2022-07-29T06:58:58.200601Z","shell.execute_reply":"2022-07-29T06:58:58.199538Z","shell.execute_reply.started":"2022-07-29T06:58:57.932412Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["FTD train shape: (849, 2), Dev shape: (283, 2), Test shape: (283, 2).\n"]}],"source":["# FTD\n","train_df = pd.read_csv(\"data/FTD-train.txt\", sep=\"\\t\", index_col=0)\n","dev_df = pd.read_csv(\"data/FTD-dev.txt\", sep = \"\\t\", index_col = 0)\n","test_df = pd.read_csv(\"data/FTD-test.txt\", sep = \"\\t\", index_col = 0)\n","\n","print(\"FTD train shape: {}, Dev shape: {}, Test shape: {}.\".format(train_df.shape, dev_df.shape, test_df.shape))"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2022-07-29T06:58:58.202595Z","iopub.status.busy":"2022-07-29T06:58:58.202243Z","iopub.status.idle":"2022-07-29T06:58:58.216854Z","shell.execute_reply":"2022-07-29T06:58:58.216026Z","shell.execute_reply.started":"2022-07-29T06:58:58.202559Z"},"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>text</th>\n","      <th>labels</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>1361</th>\n","      <td>Business continuity plans must address massive...</td>\n","      <td>7</td>\n","    </tr>\n","    <tr>\n","      <th>1605</th>\n","      <td>( INDIANAPOLIS – APRIL 16 , 2010 ) – Ash conti...</td>\n","      <td>8</td>\n","    </tr>\n","    <tr>\n","      <th>733</th>\n","      <td>Leek Friends of Israel welcome you to their we...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>495</th>\n","      <td>Npower announces further price increase Energy...</td>\n","      <td>8</td>\n","    </tr>\n","    <tr>\n","      <th>1534</th>\n","      <td>These businesses often had data , broad direct...</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                                   text  labels\n","1361  Business continuity plans must address massive...       7\n","1605  ( INDIANAPOLIS – APRIL 16 , 2010 ) – Ash conti...       8\n","733   Leek Friends of Israel welcome you to their we...       0\n","495   Npower announces further price increase Energy...       8\n","1534  These businesses often had data , broad direct...       0"]},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":["train_df.head()"]},{"cell_type":"markdown","metadata":{},"source":["## Training and saving"]},{"cell_type":"markdown","metadata":{},"source":["We will use the multilingual XLM-RoBERTa model\n","https://huggingface.co/xlm-roberta-base"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2022-07-29T06:58:58.241405Z","iopub.status.busy":"2022-07-29T06:58:58.240899Z","iopub.status.idle":"2022-07-29T06:58:58.247785Z","shell.execute_reply":"2022-07-29T06:58:58.246867Z","shell.execute_reply.started":"2022-07-29T06:58:58.241369Z"},"trusted":true},"outputs":[],"source":["import os\n","os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2022-07-29T06:58:58.249620Z","iopub.status.busy":"2022-07-29T06:58:58.249143Z","iopub.status.idle":"2022-07-29T06:58:58.262838Z","shell.execute_reply":"2022-07-29T06:58:58.261407Z","shell.execute_reply.started":"2022-07-29T06:58:58.249586Z"},"trusted":true},"outputs":[{"data":{"text/plain":["[7, 8, 0, 1, 6, 5, 2, 4, 3, 9]"]},"execution_count":13,"metadata":{},"output_type":"execute_result"}],"source":["# Create a list of labels\n","LABELS = train_df.labels.unique().tolist()\n","LABELS"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2022-07-29T06:59:24.204738Z","iopub.status.busy":"2022-07-29T06:59:24.204368Z","iopub.status.idle":"2022-07-29T06:59:27.016423Z","shell.execute_reply":"2022-07-29T06:59:27.015266Z","shell.execute_reply.started":"2022-07-29T06:59:24.204709Z"},"trusted":true},"outputs":[{"data":{"text/html":["wandb version 0.12.21 is available!  To upgrade, please run:\n"," $ pip install wandb --upgrade"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Tracking run with wandb version 0.12.11"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Run data is saved locally in <code>/home/tajak/Genre-Datasets-Comparison-1/wandb/run-20220729_110157-9v9bsb1t</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Syncing run <strong><a href=\"https://wandb.ai/tajak/FTD-learning-manual-hyperparameter-search/runs/9v9bsb1t\" target=\"_blank\">saving-trained-model</a></strong> to <a href=\"https://wandb.ai/tajak/FTD-learning-manual-hyperparameter-search\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src=\"https://wandb.ai/tajak/FTD-learning-manual-hyperparameter-search/runs/9v9bsb1t?jupyter=true\" style=\"border:none;width:100%;height:420px;display:none;\"></iframe>"],"text/plain":["<wandb.sdk.wandb_run.Run at 0x7fc344655c40>"]},"execution_count":14,"metadata":{},"output_type":"execute_result"}],"source":["# Initialize Wandb\n","wandb.init(project=\"FTD-learning-manual-hyperparameter-search\", entity=\"tajak\", name=\"saving-trained-model\")"]},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.execute_input":"2022-07-29T06:59:27.033368Z","iopub.status.busy":"2022-07-29T06:59:27.032813Z","iopub.status.idle":"2022-07-29T06:59:27.041265Z","shell.execute_reply":"2022-07-29T06:59:27.040196Z","shell.execute_reply.started":"2022-07-29T06:59:27.033342Z"},"trusted":true},"outputs":[{"data":{"text/plain":["106"]},"execution_count":16,"metadata":{},"output_type":"execute_result"}],"source":["# Calculate how many steps will each epoch have\n","# Num steps in epoch = training samples / batch size\n","steps_per_epoch = int(849/8)\n","steps_per_epoch"]},{"cell_type":"markdown","metadata":{"execution":{"iopub.execute_input":"2022-07-29T06:59:27.044920Z","iopub.status.busy":"2022-07-29T06:59:27.044530Z","iopub.status.idle":"2022-07-29T06:59:27.051272Z","shell.execute_reply":"2022-07-29T06:59:27.050401Z","shell.execute_reply.started":"2022-07-29T06:59:27.044871Z"},"trusted":true},"source":["I evaluated per every 10th epoch - per 1060 steps. I first trained the model while evaluating it to find the optimal number of epochs and then trained it again and saved it."]},{"cell_type":"code","execution_count":18,"metadata":{"execution":{"iopub.execute_input":"2022-07-29T07:00:11.772403Z","iopub.status.busy":"2022-07-29T07:00:11.771771Z","iopub.status.idle":"2022-07-29T07:00:53.140849Z","shell.execute_reply":"2022-07-29T07:00:53.139893Z","shell.execute_reply.started":"2022-07-29T07:00:11.772368Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForSequenceClassification: ['lm_head.decoder.weight', 'lm_head.bias', 'roberta.pooler.dense.weight', 'lm_head.dense.bias', 'roberta.pooler.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.weight']\n","- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","/home/tajak/anaconda3/lib/python3.9/site-packages/simpletransformers/classification/classification_model.py:459: UserWarning: use_multiprocessing automatically disabled as xlmroberta fails when using multiprocessing for feature conversion.\n","  warnings.warn(\n"]}],"source":["# Create a TransformerModel\n","roberta_base_model = ClassificationModel(\n","        \"xlmroberta\", \"xlm-roberta-base\",\n","        num_labels=len(LABELS),\n","        use_cuda=True,\n","        args= {\n","            \"overwrite_output_dir\": True,\n","            \"num_train_epochs\": 10,\n","            \"train_batch_size\":8,\n","            \"learning_rate\": 1e-5,\n","            # Use these parameters if you want to evaluate during training\n","            #\"evaluate_during_training\": True,\n","            #\"evaluate_during_training_steps\": steps_per_epoch*10,\n","            #\"evaluate_during_training_verbose\": True,\n","            #\"use_cached_eval_features\": True,\n","            #'reprocess_input_data': True,\n","            \"labels_list\": LABELS,\n","            # The following parameters are commented out because I want to save the model\n","            #\"no_cache\": True,\n","            # Disable no_save: True if you want to save the model\n","            #\"no_save\": True,\n","            \"max_seq_length\": 512,\n","            \"save_steps\": -1,\n","            # Only the trained model will be saved - to prevent filling all of the space\n","            \"save_model_every_epoch\":False,\n","            \"wandb_project\": 'FTD-learning-manual-hyperparameter-search',\n","            \"silent\": True,\n","            }\n","        )"]},{"cell_type":"code","execution_count":19,"metadata":{"execution":{"iopub.execute_input":"2022-07-29T07:04:11.442557Z","iopub.status.busy":"2022-07-29T07:04:11.441597Z","iopub.status.idle":"2022-07-29T07:05:58.527667Z","shell.execute_reply":"2022-07-29T07:05:58.526702Z","shell.execute_reply.started":"2022-07-29T07:04:11.442510Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["INFO:simpletransformers.classification.classification_utils: Converting to features started. Cache is not used.\n","INFO:simpletransformers.classification.classification_utils: Saving features into cached file cache_dir/cached_train_xlmroberta_512_10_2\n","/home/tajak/anaconda3/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"e183bbafdeb04f13a9fb93a871be0162","version_major":2,"version_minor":0},"text/plain":["Epoch:   0%|          | 0/10 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["INFO:simpletransformers.classification.classification_model: Initializing WandB run for training.\n"]},{"data":{"text/html":["Finishing last run (ID:9v9bsb1t) before initializing another..."],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["\n"]},{"data":{"text/html":["Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"373cd4711df644dfb0b6b2c74d7de653","version_major":2,"version_minor":0},"text/plain":["VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Synced <strong style=\"color:#cdcd00\">saving-trained-model</strong>: <a href=\"https://wandb.ai/tajak/FTD-learning-manual-hyperparameter-search/runs/9v9bsb1t\" target=\"_blank\">https://wandb.ai/tajak/FTD-learning-manual-hyperparameter-search/runs/9v9bsb1t</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Find logs at: <code>./wandb/run-20220729_110157-9v9bsb1t/logs</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Successfully finished last run (ID:9v9bsb1t). Initializing new run:<br/>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["wandb version 0.12.21 is available!  To upgrade, please run:\n"," $ pip install wandb --upgrade"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Tracking run with wandb version 0.12.11"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Run data is saved locally in <code>/home/tajak/Genre-Datasets-Comparison-1/wandb/run-20220729_110220-154unzki</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Syncing run <strong><a href=\"https://wandb.ai/tajak/FTD-learning-manual-hyperparameter-search/runs/154unzki\" target=\"_blank\">breezy-wave-39</a></strong> to <a href=\"https://wandb.ai/tajak/FTD-learning-manual-hyperparameter-search\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"8993400e93434cdba2737054ac65d278","version_major":2,"version_minor":0},"text/plain":["Running Epoch 0 of 10:   0%|          | 0/107 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"4d57e02e93a64c48a5d644b58460abd8","version_major":2,"version_minor":0},"text/plain":["Running Epoch 1 of 10:   0%|          | 0/107 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"f291f01ca5ad4740b7858a0e964d3395","version_major":2,"version_minor":0},"text/plain":["Running Epoch 2 of 10:   0%|          | 0/107 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"b605456fdb6d46d797ae2f3b2aae45d1","version_major":2,"version_minor":0},"text/plain":["Running Epoch 3 of 10:   0%|          | 0/107 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"7ee1399eacef4657966de3ac224d180c","version_major":2,"version_minor":0},"text/plain":["Running Epoch 4 of 10:   0%|          | 0/107 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"0c3e4b0c01d147aab2f56e0f2eb5f545","version_major":2,"version_minor":0},"text/plain":["Running Epoch 5 of 10:   0%|          | 0/107 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"23772133fe9542baab940251a57c80db","version_major":2,"version_minor":0},"text/plain":["Running Epoch 6 of 10:   0%|          | 0/107 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"2da828c257bd45e2ba3c41ca7b4251df","version_major":2,"version_minor":0},"text/plain":["Running Epoch 7 of 10:   0%|          | 0/107 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"2bb802a003fc47e7ab7bc4e298e945c6","version_major":2,"version_minor":0},"text/plain":["Running Epoch 8 of 10:   0%|          | 0/107 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"615517eac6274b01b109f27cb5ed7aed","version_major":2,"version_minor":0},"text/plain":["Running Epoch 9 of 10:   0%|          | 0/107 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["INFO:simpletransformers.classification.classification_model: Training of xlmroberta model complete. Saved to outputs/.\n"]},{"data":{"text/plain":["(1070, 0.9864452896831192)"]},"execution_count":19,"metadata":{},"output_type":"execute_result"}],"source":["# Train the model\n","roberta_base_model.train_model(train_df)"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2022-07-29T07:18:54.719888Z","iopub.status.busy":"2022-07-29T07:18:54.719040Z","iopub.status.idle":"2022-07-29T07:19:29.809237Z","shell.execute_reply":"2022-07-29T07:19:29.808256Z","shell.execute_reply.started":"2022-07-29T07:18:54.719856Z"},"trusted":true},"outputs":[{"data":{"text/html":["wandb version 0.12.21 is available!  To upgrade, please run:\n"," $ pip install wandb --upgrade"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Tracking run with wandb version 0.12.11"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Run data is saved locally in <code>/home/tajak/Genre-Datasets-Comparison/Genre-Datasets-Comparison/wandb/run-20220802_114542-2yb5b4co</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Syncing run <strong><a href=\"https://wandb.ai/tajak/FTD-learning-manual-hyperparameter-search/runs/2yb5b4co\" target=\"_blank\">saving-trained-model</a></strong> to <a href=\"https://wandb.ai/tajak/FTD-learning-manual-hyperparameter-search\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./artifacts/FTD-classifier:v1)... Done. 1.9s\n"]},{"data":{"text/plain":["<wandb.sdk.wandb_artifacts.Artifact at 0x7fe257743d60>"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["# Save the trained model to Wandb\n","run = wandb.init(project=\"FTD-learning-manual-hyperparameter-search\", entity=\"tajak\", name=\"saving-trained-model\")\n","trained_model_artifact = wandb.Artifact(\"FTD-classifier\", type=\"model\", description=\"a model trained on the FTD dataset\")\n","trained_model_artifact.add_dir(\"artifacts/FTD-classifier:v1\")\n","run.log_artifact(trained_model_artifact)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2022-07-29T06:59:40.742862Z","iopub.status.idle":"2022-07-29T06:59:40.743686Z","shell.execute_reply":"2022-07-29T06:59:40.743449Z","shell.execute_reply.started":"2022-07-29T06:59:40.743424Z"},"trusted":true},"outputs":[],"source":["# Clean the GPU cache\n","cuda.select_device(0)\n","cuda.close()\n","cuda.select_device(0)\n","torch.cuda.empty_cache()"]}],"metadata":{"kernelspec":{"display_name":"Python 3.9.7 ('base': conda)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.7"},"vscode":{"interpreter":{"hash":"7e373e41fe05b496006fe2fc132d7af19f1d513370c44925a0044a5f3ee41336"}}},"nbformat":4,"nbformat_minor":4}
