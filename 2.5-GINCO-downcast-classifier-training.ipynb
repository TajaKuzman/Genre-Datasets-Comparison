{"cells":[{"cell_type":"markdown","metadata":{},"source":["Import all necessary libraries and install everything you need for training:"]},{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2022-08-02T06:32:41.558575Z","iopub.status.busy":"2022-08-02T06:32:41.558215Z","iopub.status.idle":"2022-08-02T06:32:45.499427Z","shell.execute_reply":"2022-08-02T06:32:45.498234Z","shell.execute_reply.started":"2022-08-02T06:32:41.558494Z"},"trusted":true},"outputs":[],"source":["# install the libraries necessary for data wrangling, prediction and result analysis\n","import json\n","import numpy as np\n","import pandas as pd\n","import logging\n","import matplotlib.pyplot as plt\n","from sklearn import metrics\n","from sklearn.metrics import classification_report, confusion_matrix, f1_score,precision_score, recall_score\n","import torch\n","from numba import cuda\n","from sklearn.model_selection import train_test_split\n","from sklearn.dummy import DummyClassifier"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2022-08-02T06:32:50.759354Z","iopub.status.busy":"2022-08-02T06:32:50.758784Z","iopub.status.idle":"2022-08-02T06:33:01.638619Z","shell.execute_reply":"2022-08-02T06:33:01.637174Z","shell.execute_reply.started":"2022-08-02T06:32:50.759321Z"},"trusted":true},"outputs":[],"source":["# Install transformers\n","# (this needs to be done on Kaggle each time you start the session)\n","#!pip install -q transformers"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2022-08-02T06:33:01.642547Z","iopub.status.busy":"2022-08-02T06:33:01.641888Z","iopub.status.idle":"2022-08-02T06:33:33.210150Z","shell.execute_reply":"2022-08-02T06:33:33.209136Z","shell.execute_reply.started":"2022-08-02T06:33:01.642502Z"},"trusted":true},"outputs":[],"source":["# Install the simpletransformers\n","#!pip install -q simpletransformers\n","from simpletransformers.classification import ClassificationModel"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2022-08-02T06:33:33.212469Z","iopub.status.busy":"2022-08-02T06:33:33.211577Z","iopub.status.idle":"2022-08-02T06:33:42.330151Z","shell.execute_reply":"2022-08-02T06:33:42.328978Z","shell.execute_reply.started":"2022-08-02T06:33:33.212428Z"},"trusted":true},"outputs":[],"source":["# Install wandb\n","#!pip install -q wandb"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2022-08-02T06:33:42.333657Z","iopub.status.busy":"2022-08-02T06:33:42.333143Z","iopub.status.idle":"2022-08-02T06:33:42.341586Z","shell.execute_reply":"2022-08-02T06:33:42.340744Z","shell.execute_reply.started":"2022-08-02T06:33:42.333617Z"},"trusted":true},"outputs":[],"source":["import wandb"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2022-08-02T06:33:59.521419Z","iopub.status.busy":"2022-08-02T06:33:59.520826Z","iopub.status.idle":"2022-08-02T06:34:09.243258Z","shell.execute_reply":"2022-08-02T06:34:09.242144Z","shell.execute_reply.started":"2022-08-02T06:33:59.521385Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mtajak\u001b[0m (use `wandb login --relogin` to force relogin)\n"]},{"data":{"text/plain":["True"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["# Login to wandb\n","wandb.login()"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2022-07-29T06:58:57.624647Z","iopub.status.busy":"2022-07-29T06:58:57.624034Z","iopub.status.idle":"2022-07-29T06:58:57.929939Z","shell.execute_reply":"2022-07-29T06:58:57.928897Z","shell.execute_reply.started":"2022-07-29T06:58:57.624608Z"},"trusted":true},"outputs":[],"source":["# Clean the GPU cache\n","\n","cuda.select_device(0)\n","cuda.close()\n","cuda.select_device(0)\n","torch.cuda.empty_cache()\n"]},{"cell_type":"markdown","metadata":{},"source":["### Import the data"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2022-08-02T06:34:55.973675Z","iopub.status.busy":"2022-08-02T06:34:55.972975Z","iopub.status.idle":"2022-08-02T06:34:56.090212Z","shell.execute_reply":"2022-08-02T06:34:56.089098Z","shell.execute_reply.started":"2022-08-02T06:34:55.973641Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["GINCO train shape: (601, 2), Dev shape: (201, 2), Test shape: (200, 2).\n"]}],"source":["# GINCO full set (all labels except the labels with less than 10 instances)\n","train_df = pd.read_csv(\"data-splits/GINCO-downcast-train.csv\", index_col = 0)\n","dev_df = pd.read_csv(\"data-splits/GINCO-downcast-dev.csv\", index_col = 0)\n","test_df = pd.read_csv(\"data-splits/GINCO-downcast-test.csv\", index_col = 0)\n","\n","print(\"GINCO train shape: {}, Dev shape: {}, Test shape: {}.\".format(train_df.shape, dev_df.shape, test_df.shape))"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2022-08-02T06:35:22.255577Z","iopub.status.busy":"2022-08-02T06:35:22.254707Z","iopub.status.idle":"2022-08-02T06:35:22.272023Z","shell.execute_reply":"2022-08-02T06:35:22.270867Z","shell.execute_reply.started":"2022-08-02T06:35:22.255544Z"},"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>text</th>\n","      <th>labels</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>324</th>\n","      <td>Ja tudi meni se zdi grdo, da moram razlagati z...</td>\n","      <td>Forum</td>\n","    </tr>\n","    <tr>\n","      <th>253</th>\n","      <td>Bo kolesarjenje postalo del zimskih športov? &lt;...</td>\n","      <td>News/Reporting</td>\n","    </tr>\n","    <tr>\n","      <th>674</th>\n","      <td>Teptanje ustave namesto prevzema odgovornosti ...</td>\n","      <td>Opinion/Argumentation</td>\n","    </tr>\n","    <tr>\n","      <th>600</th>\n","      <td>Cena odtisa za Canon i990 &lt;p/&gt; Zdaj smo končno...</td>\n","      <td>Opinion/Argumentation</td>\n","    </tr>\n","    <tr>\n","      <th>460</th>\n","      <td>Veronikin tek je idealna priložnost za priprav...</td>\n","      <td>Promotion</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                                  text                 labels\n","324  Ja tudi meni se zdi grdo, da moram razlagati z...                  Forum\n","253  Bo kolesarjenje postalo del zimskih športov? <...         News/Reporting\n","674  Teptanje ustave namesto prevzema odgovornosti ...  Opinion/Argumentation\n","600  Cena odtisa za Canon i990 <p/> Zdaj smo končno...  Opinion/Argumentation\n","460  Veronikin tek je idealna priložnost za priprav...              Promotion"]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["train_df.head()"]},{"cell_type":"markdown","metadata":{},"source":["## Training and saving"]},{"cell_type":"markdown","metadata":{},"source":["We will use the multilingual XLM-RoBERTa model\n","https://huggingface.co/xlm-roberta-base"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2022-08-02T06:35:28.967983Z","iopub.status.busy":"2022-08-02T06:35:28.967597Z","iopub.status.idle":"2022-08-02T06:35:28.972711Z","shell.execute_reply":"2022-08-02T06:35:28.971619Z","shell.execute_reply.started":"2022-08-02T06:35:28.967950Z"},"trusted":true},"outputs":[],"source":["import os\n","os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2022-08-02T06:35:31.559640Z","iopub.status.busy":"2022-08-02T06:35:31.559274Z","iopub.status.idle":"2022-08-02T06:35:31.569636Z","shell.execute_reply":"2022-08-02T06:35:31.568456Z","shell.execute_reply.started":"2022-08-02T06:35:31.559610Z"},"trusted":true},"outputs":[{"data":{"text/plain":["['Forum',\n"," 'News/Reporting',\n"," 'Opinion/Argumentation',\n"," 'Promotion',\n"," 'Information/Explanation',\n"," 'List of Summaries/Excerpts',\n"," 'Other',\n"," 'Instruction',\n"," 'Legal/Regulation']"]},"execution_count":10,"metadata":{},"output_type":"execute_result"}],"source":["# Create a list of labels\n","LABELS = train_df.labels.unique().tolist()\n","LABELS"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2022-08-02T06:35:53.134377Z","iopub.status.busy":"2022-08-02T06:35:53.133686Z","iopub.status.idle":"2022-08-02T06:35:56.389020Z","shell.execute_reply":"2022-08-02T06:35:56.388060Z","shell.execute_reply.started":"2022-08-02T06:35:53.134341Z"},"trusted":true},"outputs":[{"data":{"text/html":["wandb version 0.12.21 is available!  To upgrade, please run:\n"," $ pip install wandb --upgrade"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Tracking run with wandb version 0.12.11"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Run data is saved locally in <code>/home/tajak/Genre-Datasets-Comparison/Genre-Datasets-Comparison/wandb/run-20220802_143049-2lwt395m</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Syncing run <strong><a href=\"https://wandb.ai/tajak/GINCO-hyperparameter-search/runs/2lwt395m\" target=\"_blank\">GINCO-downcast-hyperparameter-search</a></strong> to <a href=\"https://wandb.ai/tajak/GINCO-hyperparameter-search\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src=\"https://wandb.ai/tajak/GINCO-hyperparameter-search/runs/2lwt395m?jupyter=true\" style=\"border:none;width:100%;height:420px;display:none;\"></iframe>"],"text/plain":["<wandb.sdk.wandb_run.Run at 0x7fd133284100>"]},"execution_count":11,"metadata":{},"output_type":"execute_result"}],"source":["# Initialize Wandb\n","wandb.init(project=\"GINCO-hyperparameter-search\", name=\"GINCO-downcast-hyperparameter-search\")"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2022-08-02T06:35:59.158607Z","iopub.status.busy":"2022-08-02T06:35:59.158230Z","iopub.status.idle":"2022-08-02T06:35:59.169780Z","shell.execute_reply":"2022-08-02T06:35:59.168283Z","shell.execute_reply.started":"2022-08-02T06:35:59.158575Z"},"trusted":true},"outputs":[{"data":{"text/plain":["75"]},"execution_count":12,"metadata":{},"output_type":"execute_result"}],"source":["# Calculate how many steps will each epoch have\n","# Num steps in epoch = training samples / batch size\n","steps_per_epoch = int(601/8)\n","steps_per_epoch"]},{"cell_type":"markdown","metadata":{"execution":{"iopub.execute_input":"2022-07-29T06:59:27.044920Z","iopub.status.busy":"2022-07-29T06:59:27.044530Z","iopub.status.idle":"2022-07-29T06:59:27.051272Z","shell.execute_reply":"2022-07-29T06:59:27.050401Z","shell.execute_reply.started":"2022-07-29T06:59:27.044871Z"},"trusted":true},"source":["I evaluated per every 10th epoch - per 750 steps. I first trained the model while evaluating it to find the optimal number of epochs. As previously already 20 epochs was enough, I will train only up to 30 epochs."]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2022-08-02T06:36:31.655009Z","iopub.status.busy":"2022-08-02T06:36:31.653964Z","iopub.status.idle":"2022-08-02T06:37:39.757493Z","shell.execute_reply":"2022-08-02T06:37:39.756524Z","shell.execute_reply.started":"2022-08-02T06:36:31.654965Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForSequenceClassification: ['lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.bias', 'roberta.pooler.dense.bias', 'roberta.pooler.dense.weight', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.weight']\n","- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","/home/tajak/anaconda3/lib/python3.9/site-packages/simpletransformers/classification/classification_model.py:459: UserWarning: use_multiprocessing automatically disabled as xlmroberta fails when using multiprocessing for feature conversion.\n","  warnings.warn(\n","/home/tajak/anaconda3/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n"]},{"data":{"text/html":["Finishing last run (ID:2lwt395m) before initializing another..."],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["\n"]},{"data":{"text/html":["Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"c3b0d91758c947c38a44cb1a5b3a47d3","version_major":2,"version_minor":0},"text/plain":["VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Synced <strong style=\"color:#cdcd00\">GINCO-downcast-hyperparameter-search</strong>: <a href=\"https://wandb.ai/tajak/GINCO-hyperparameter-search/runs/2lwt395m\" target=\"_blank\">https://wandb.ai/tajak/GINCO-hyperparameter-search/runs/2lwt395m</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Find logs at: <code>./wandb/run-20220802_143049-2lwt395m/logs</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Successfully finished last run (ID:2lwt395m). Initializing new run:<br/>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["wandb version 0.12.21 is available!  To upgrade, please run:\n"," $ pip install wandb --upgrade"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Tracking run with wandb version 0.12.11"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Run data is saved locally in <code>/home/tajak/Genre-Datasets-Comparison/Genre-Datasets-Comparison/wandb/run-20220802_143215-3olmxfnq</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Syncing run <strong><a href=\"https://wandb.ai/tajak/GINCO-hyperparameter-search/runs/3olmxfnq\" target=\"_blank\">laced-energy-17</a></strong> to <a href=\"https://wandb.ai/tajak/GINCO-hyperparameter-search\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m~/anaconda3/lib/python3.9/multiprocessing/pool.py\u001b[0m in \u001b[0;36mnext\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    852\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 853\u001b[0;31m                 \u001b[0mitem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_items\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpopleft\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    854\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mIndexError\u001b[0m: pop from an empty deque","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_26679/626609168.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;31m# Train the model and evaluate during training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m \u001b[0mroberta_base_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdev_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/simpletransformers/classification/classification_model.py\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(self, train_df, multi_label, output_dir, show_running_loss, args, eval_df, verbose, **kwargs)\u001b[0m\n\u001b[1;32m    603\u001b[0m         \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmakedirs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexist_ok\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    604\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 605\u001b[0;31m         global_step, training_details = self.train(\n\u001b[0m\u001b[1;32m    606\u001b[0m             \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    607\u001b[0m             \u001b[0moutput_dir\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/simpletransformers/classification/classification_model.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, train_dataloader, output_dir, multi_label, show_running_loss, eval_df, test_df, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   1141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1142\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate_during_training\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate_each_epoch\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1143\u001b[0;31m                 results, _, _ = self.eval_model(\n\u001b[0m\u001b[1;32m   1144\u001b[0m                     \u001b[0meval_df\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1145\u001b[0m                     \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate_during_training_verbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/simpletransformers/classification/classification_model.py\u001b[0m in \u001b[0;36meval_model\u001b[0;34m(self, eval_df, multi_label, output_dir, verbose, silent, wandb_log, **kwargs)\u001b[0m\n\u001b[1;32m   1330\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_move_model_to_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1331\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1332\u001b[0;31m         result, model_outputs, wrong_preds = self.evaluate(\n\u001b[0m\u001b[1;32m   1333\u001b[0m             \u001b[0meval_df\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1334\u001b[0m             \u001b[0moutput_dir\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/simpletransformers/classification/classification_model.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, eval_df, output_dir, multi_label, prefix, verbose, silent, wandb_log, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m                 )\n\u001b[1;32m   1438\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m                 eval_dataset = self.load_and_cache_examples(\n\u001b[0m\u001b[1;32m   1440\u001b[0m                     \u001b[0meval_examples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msilent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msilent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m                 )\n","\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/simpletransformers/classification/classification_model.py\u001b[0m in \u001b[0;36mload_and_cache_examples\u001b[0;34m(self, examples, evaluate, no_cache, multi_label, verbose, silent)\u001b[0m\n\u001b[1;32m   1797\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1798\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1799\u001b[0;31m             dataset = ClassificationDataset(\n\u001b[0m\u001b[1;32m   1800\u001b[0m                 \u001b[0mexamples\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1801\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/simpletransformers/classification/classification_utils.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, tokenizer, args, mode, multi_label, output_mode, no_cache)\u001b[0m\n\u001b[1;32m    276\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mClassificationDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    277\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmulti_label\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_mode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mno_cache\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 278\u001b[0;31m         self.examples, self.labels = build_classification_dataset(\n\u001b[0m\u001b[1;32m    279\u001b[0m             \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmulti_label\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_mode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mno_cache\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    280\u001b[0m         )\n","\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/simpletransformers/classification/classification_utils.py\u001b[0m in \u001b[0;36mbuild_classification_dataset\u001b[0;34m(data, tokenizer, args, mode, multi_label, output_mode, no_cache)\u001b[0m\n\u001b[1;32m    243\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mPool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_count\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 245\u001b[0;31m                 examples = list(\n\u001b[0m\u001b[1;32m    246\u001b[0m                     tqdm(\n\u001b[1;32m    247\u001b[0m                         \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreprocess_data_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/tqdm/notebook.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    255\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 257\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtqdm_notebook\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    258\u001b[0m                 \u001b[0;31m# return super(tqdm...) will not catch exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/tqdm/std.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1166\u001b[0m         \u001b[0;31m# (note: keep this check outside the loop for performance)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1167\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1168\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1169\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1170\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/anaconda3/lib/python3.9/multiprocessing/pool.py\u001b[0m in \u001b[0;36mnext\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    856\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    857\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 858\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    859\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    860\u001b[0m                     \u001b[0mitem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_items\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpopleft\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/anaconda3/lib/python3.9/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    310\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    311\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 312\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    313\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["# Create a TransformerModel and evaluate during training\n","epoch = 30\n","\n","roberta_base_model = ClassificationModel(\n","        \"xlmroberta\", \"xlm-roberta-base\",\n","        num_labels=len(LABELS),\n","        use_cuda=True,\n","        args= {\n","            \"overwrite_output_dir\": True,\n","            \"num_train_epochs\": epoch,\n","            \"train_batch_size\":8,\n","            \"learning_rate\": 1e-5,\n","            # Use these parameters if you want to evaluate during training\n","            \"evaluate_during_training\": True,\n","            \"evaluate_during_training_steps\": steps_per_epoch*10,\n","            \"evaluate_during_training_verbose\": True,\n","            \"use_cached_eval_features\": True,\n","            'reprocess_input_data': True,\n","            \"labels_list\": LABELS,\n","            # The following parameters (no_cache, no_save) are commented out if I want to save the model\n","            \"no_cache\": True,\n","            # Disable no_save: True if you want to save the model\n","            \"no_save\": True,\n","            \"max_seq_length\": 512,\n","            \"save_steps\": -1,\n","            # Only the trained model will be saved - to prevent filling all of the space\n","            \"save_model_every_epoch\":False,\n","            \"wandb_project\": 'GINCO-hyperparameter-search',\n","            \"silent\": True,\n","            }\n","        )\n","\n","# Train the model and evaluate during training\n","roberta_base_model.train_model(train_df, eval_df = dev_df)"]},{"cell_type":"markdown","metadata":{},"source":["Evaluation during training showed that the number of epochs before the eval_loss starts rising is somewhere between epochs 12 and 25. We then trained the model for epochs 12, 15, 20 and 25 to find the optimum number."]},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.execute_input":"2022-08-02T07:50:52.769657Z","iopub.status.busy":"2022-08-02T07:50:52.769302Z","iopub.status.idle":"2022-08-02T07:50:52.775031Z","shell.execute_reply":"2022-08-02T07:50:52.774086Z","shell.execute_reply.started":"2022-08-02T07:50:52.769626Z"},"trusted":true},"outputs":[],"source":["# Create a file to save results into (you can find it under Data: Output). Be careful, run this step only once to not overwrite the results file.\n","results = []\n","\n","with open(\"GINCO-Experiments-Results.json\", \"w\") as results_file:\n","    json.dump(results,results_file, indent= \"\")"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2022-08-02T07:51:14.620567Z","iopub.status.busy":"2022-08-02T07:51:14.619750Z","iopub.status.idle":"2022-08-02T07:51:14.628329Z","shell.execute_reply":"2022-08-02T07:51:14.627218Z","shell.execute_reply.started":"2022-08-02T07:51:14.620532Z"},"trusted":true},"outputs":[{"data":{"text/plain":["5"]},"execution_count":14,"metadata":{},"output_type":"execute_result"}],"source":["# Open the main results file:\n","\n","previous_results_file = open(\"results/GINCO-Experiments-Results.json\")\n","previous_results = json.load(previous_results_file)\n","len(previous_results)"]},{"cell_type":"code","execution_count":19,"metadata":{"execution":{"iopub.execute_input":"2022-08-02T07:54:55.910517Z","iopub.status.busy":"2022-08-02T07:54:55.910049Z","iopub.status.idle":"2022-08-02T07:54:55.938609Z","shell.execute_reply":"2022-08-02T07:54:55.936969Z","shell.execute_reply.started":"2022-08-02T07:54:55.910478Z"},"trusted":true},"outputs":[],"source":["def testing(test_df, test_name, epoch):\n","    \"\"\"\n","    This function takes the test dataset and applies the trained model on it to infer predictions.\n","    It also prints and saves a confusion matrix, calculates the F1 scores and saves the results in a list of results.\n","\n","    Args:\n","    - test_df (pandas DataFrame)\n","    - test_name\n","    - epoch: num_train_epochs\n","    \"\"\"\n","    # Get the true labels\n","    y_true = test_df.labels\n","\n","    model = roberta_base_model\n","    \n","    # Calculate the model's predictions on test\n","    def make_prediction(input_string):\n","        return model.predict([input_string])[0][0]\n","\n","    y_pred = test_df.text.apply(make_prediction)\n","\n","    # Calculate the scores\n","    macro = f1_score(y_true, y_pred, labels=LABELS, average=\"macro\")\n","    micro = f1_score(y_true, y_pred, labels=LABELS,  average=\"micro\")\n","    print(f\"Macro f1: {macro:0.3}, Micro f1: {micro:0.3}\")\n","\n","    # Plot the confusion matrix:\n","    cm = confusion_matrix(y_true, y_pred, labels=LABELS)\n","    plt.figure(figsize=(9, 9))\n","    plt.imshow(cm, cmap=\"Oranges\")\n","    for (i, j), z in np.ndenumerate(cm):\n","        plt.text(j, i, '{:d}'.format(z), ha='center', va='center')\n","    classNames = LABELS\n","    plt.ylabel('True label')\n","    plt.xlabel('Predicted label')\n","    tick_marks = np.arange(len(classNames))\n","    plt.xticks(tick_marks, classNames, rotation=90)\n","    plt.yticks(tick_marks, classNames)\n","    plt.title(f\"{test_name}\")\n","\n","    plt.tight_layout()\n","    fig1 = plt.gcf()\n","    plt.show()\n","    plt.draw()\n","    fig1.savefig(f\"Confusion-matrix-{test_name}.png\",dpi=100)\n","\n","    # Save the results:\n","    rezdict = {\n","        \"experiment\": test_name,\n","        \"num_train_epochs\": epoch,\n","        \"train_batch_size\":8,\n","        \"learning_rate\": 1e-5,\n","        \"microF1\": micro,\n","        \"macroF1\": macro,\n","        \"y_true\": y_true.to_dict(),\n","        \"y_pred\": y_pred.to_dict(),\n","        }\n","    previous_results.append(rezdict)\n","\n","    #Save intermediate results (just in case)\n","    backup = []\n","    backup.append(rezdict)\n","    with open(f\"backup-results-{test_name}.json\", \"w\") as backup_file:\n","        json.dump(backup,backup_file, indent= \"\")"]},{"cell_type":"code","execution_count":21,"metadata":{"execution":{"iopub.execute_input":"2022-08-02T07:57:34.631892Z","iopub.status.busy":"2022-08-02T07:57:34.631509Z","iopub.status.idle":"2022-08-02T09:06:36.643312Z","shell.execute_reply":"2022-08-02T09:06:36.642360Z","shell.execute_reply.started":"2022-08-02T07:57:34.631860Z"},"trusted":true},"outputs":[],"source":["# Train the model for various epochs to find the optimum number\n","epochs = [1, 12, 15, 20, 25]\n","\n","for epoch in epochs:\n","    roberta_base_model = ClassificationModel(\n","                \"xlmroberta\", \"xlm-roberta-base\",\n","                num_labels=len(LABELS),\n","                use_cuda=True,\n","                args= {\n","                    \"overwrite_output_dir\": True,\n","                    \"num_train_epochs\": epoch,\n","                    \"train_batch_size\":8,\n","                    \"learning_rate\": 1e-5,\n","                    \"labels_list\": LABELS,\n","                    # The following parameters (no_cache, no_save) are commented out if I want to save the model\n","                    \"no_cache\": True,\n","                    # Disable no_save: True if you want to save the model\n","                    \"no_save\": True,\n","                    \"max_seq_length\": 512,\n","                    \"save_steps\": -1,\n","                    # Only the trained model will be saved - to prevent filling all of the space\n","                    \"save_model_every_epoch\":False,\n","                    \"wandb_project\": 'GINCO-hyperparameter-search',\n","                    \"silent\": True,\n","                    }\n","                )\n","\n","    # Train the model\n","    roberta_base_model.train_model(train_df)\n","    \n","    # Test the model on dev_df\n","    testing(dev_df, f\"Dev-epoch-search:{epoch}\", epoch)"]},{"cell_type":"code","execution_count":22,"metadata":{"execution":{"iopub.execute_input":"2022-08-02T09:16:05.660652Z","iopub.status.busy":"2022-08-02T09:16:05.659980Z","iopub.status.idle":"2022-08-02T09:16:05.696763Z","shell.execute_reply":"2022-08-02T09:16:05.695715Z","shell.execute_reply.started":"2022-08-02T09:16:05.660615Z"},"trusted":true},"outputs":[],"source":["# Compare the results by creating a dataframe from the previous_results dictionary:\n","results_df = pd.DataFrame(previous_results)\n","\n","results_df"]},{"cell_type":"markdown","metadata":{},"source":["The experiments revealed that the optimum number of epochs is 20."]},{"cell_type":"code","execution_count":23,"metadata":{"execution":{"iopub.execute_input":"2022-08-02T09:16:39.883369Z","iopub.status.busy":"2022-08-02T09:16:39.883008Z","iopub.status.idle":"2022-08-02T09:16:39.897367Z","shell.execute_reply":"2022-08-02T09:16:39.896406Z","shell.execute_reply.started":"2022-08-02T09:16:39.883340Z"},"trusted":true},"outputs":[],"source":["# Save the file with updated results.\n","with open(\"GINCO-Experiments-Results.json\", \"w\") as results_file:\n","    json.dump(previous_results,results_file, indent= \"\")"]},{"cell_type":"markdown","metadata":{},"source":["Train the model and save it."]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForSequenceClassification: ['lm_head.decoder.weight', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'roberta.pooler.dense.weight', 'lm_head.dense.weight', 'roberta.pooler.dense.bias', 'lm_head.layer_norm.weight']\n","- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","/home/tajak/anaconda3/lib/python3.9/site-packages/simpletransformers/classification/classification_model.py:459: UserWarning: use_multiprocessing automatically disabled as xlmroberta fails when using multiprocessing for feature conversion.\n","  warnings.warn(\n"]}],"source":["# Create a TransformerModel\n","roberta_base_model = ClassificationModel(\n","        \"xlmroberta\", \"xlm-roberta-base\",\n","        num_labels=len(LABELS),\n","        use_cuda=True,\n","        args= {\n","            \"overwrite_output_dir\": True,\n","            \"num_train_epochs\": 20,\n","            \"train_batch_size\":8,\n","            \"learning_rate\": 1e-5,\n","            \"labels_list\": LABELS,\n","            # The following parameters are commented out because I want to save the model\n","            #\"no_cache\": True,\n","            # Disable no_save: True if you want to save the model\n","            #\"no_save\": True,\n","            \"max_seq_length\": 512,\n","            \"save_steps\": -1,\n","            # Only the trained model will be saved - to prevent filling all of the space\n","            \"save_model_every_epoch\":False,\n","            \"wandb_project\": 'GINCO-hyperparameter-search',\n","            \"silent\": True,\n","            }\n","        )"]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["/home/tajak/anaconda3/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n"]},{"data":{"text/html":["Finishing last run (ID:1w1k6mi6) before initializing another..."],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["\n"]},{"data":{"text/html":["Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"548a8913d45843fbb7ce6e93b15a44f6","version_major":2,"version_minor":0},"text/plain":["VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Synced <strong style=\"color:#cdcd00\">training-and-saving-the-model</strong>: <a href=\"https://wandb.ai/tajak/GINCO-hyperparameter-search/runs/1w1k6mi6\" target=\"_blank\">https://wandb.ai/tajak/GINCO-hyperparameter-search/runs/1w1k6mi6</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Find logs at: <code>./wandb/run-20220802_112508-1w1k6mi6/logs</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Successfully finished last run (ID:1w1k6mi6). Initializing new run:<br/>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["wandb version 0.12.21 is available!  To upgrade, please run:\n"," $ pip install wandb --upgrade"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Tracking run with wandb version 0.12.11"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Run data is saved locally in <code>/home/tajak/Genre-Datasets-Comparison/Genre-Datasets-Comparison/wandb/run-20220802_112848-35feaylp</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Syncing run <strong><a href=\"https://wandb.ai/tajak/GINCO-hyperparameter-search/runs/35feaylp\" target=\"_blank\">decent-blaze-11</a></strong> to <a href=\"https://wandb.ai/tajak/GINCO-hyperparameter-search\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["(1460, 1.138362426320984)"]},"execution_count":16,"metadata":{},"output_type":"execute_result"}],"source":["# Train the model\n","roberta_base_model.train_model(train_df)"]},{"cell_type":"code","execution_count":17,"metadata":{"execution":{"iopub.execute_input":"2022-07-29T07:18:54.719888Z","iopub.status.busy":"2022-07-29T07:18:54.719040Z","iopub.status.idle":"2022-07-29T07:19:29.809237Z","shell.execute_reply":"2022-07-29T07:19:29.808256Z","shell.execute_reply.started":"2022-07-29T07:18:54.719856Z"},"trusted":true},"outputs":[{"data":{"text/html":["Finishing last run (ID:35feaylp) before initializing another..."],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["\n"]},{"data":{"text/html":["Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"3d6f4c12ddcf42e3ba47b9e6fff1dfb5","version_major":2,"version_minor":0},"text/plain":["VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<style>\n","    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n","    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n","    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n","    </style>\n","<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Training loss</td><td>██▇▆█▅▄▄▃▆▃▃▁▃▃▂▃▂▂▁▂▂▃▁▂▁▁▁▁</td></tr><tr><td>global_step</td><td>▁▁▁▂▂▂▃▃▃▃▃▄▄▄▅▅▅▅▅▆▆▆▇▇▇▇▇██</td></tr><tr><td>lr</td><td>▅██▇▇▇▇▆▆▆▆▅▅▅▅▄▄▄▄▃▃▃▃▂▂▂▂▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Training loss</td><td>0.31044</td></tr><tr><td>global_step</td><td>1450</td></tr><tr><td>lr</td><td>0.0</td></tr></table><br/></div></div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Synced <strong style=\"color:#cdcd00\">decent-blaze-11</strong>: <a href=\"https://wandb.ai/tajak/GINCO-hyperparameter-search/runs/35feaylp\" target=\"_blank\">https://wandb.ai/tajak/GINCO-hyperparameter-search/runs/35feaylp</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Find logs at: <code>./wandb/run-20220802_112848-35feaylp/logs</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Successfully finished last run (ID:35feaylp). Initializing new run:<br/>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["wandb version 0.12.21 is available!  To upgrade, please run:\n"," $ pip install wandb --upgrade"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Tracking run with wandb version 0.12.11"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Run data is saved locally in <code>/home/tajak/Genre-Datasets-Comparison/Genre-Datasets-Comparison/wandb/run-20220802_114115-n1sokgd4</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Syncing run <strong><a href=\"https://wandb.ai/tajak/GINCO-hyperparameter-search/runs/n1sokgd4\" target=\"_blank\">saving-trained-model</a></strong> to <a href=\"https://wandb.ai/tajak/GINCO-hyperparameter-search\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./outputs)... Done. 2.8s\n"]},{"data":{"text/plain":["<wandb.sdk.wandb_artifacts.Artifact at 0x7efe702fe790>"]},"execution_count":17,"metadata":{},"output_type":"execute_result"}],"source":["# Save the trained model to Wandb\n","run = wandb.init(project=\"GINCO-hyperparameter-search\", entity=\"tajak\", name=\"saving-trained-model\")\n","trained_model_artifact = wandb.Artifact(\"GINCO-full-set-classifier\", type=\"model\", description=\"a model trained on the (Slovene) GINCO dataset with the full set of labels (only the labels with less than 10 instances were excluded. There are 17 labels: 'Promotion of a Product', 'Forum', 'Instruction', 'News/Reporting', 'Opinionated News', 'Other', 'List of Summaries/Excerpts', 'Invitation', 'Opinion/Argumentation', 'Legal/Regulation', 'Information/Explanation', 'Review', 'Promotion of Services', 'Promotion', 'Announcement', 'Call', 'Correspondence'\")\n","trained_model_artifact.add_dir(\"outputs\")\n","run.log_artifact(trained_model_artifact)"]}],"metadata":{"kernelspec":{"display_name":"Python 3.9.7 ('base')","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.7"},"vscode":{"interpreter":{"hash":"7e373e41fe05b496006fe2fc132d7af19f1d513370c44925a0044a5f3ee41336"}}},"nbformat":4,"nbformat_minor":4}
